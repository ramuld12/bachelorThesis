{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def CleanText(text):\n",
    "#    text = re.sub(r'''[\\[|\\]]''', \"\", text).split()\n",
    "#    text = np.array(text, dtype=\"float64\")\n",
    "#    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanText(text):\n",
    "    text = text.lower() #Turn all text entries into lower-case\n",
    "    text = re.sub(r'''(https?:\\/\\/www\\.|https?:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,3}[-a-zA-Z0-9()@:%_\\+.~#?&\\//=<>]*''', \"<URL>\", text)\n",
    "    #Replace URL with tag\n",
    "    text = re.sub(r'''[0-9]+[/\\-.]+[0-9]+[/\\-.]+[0-9]+''', \"<DATE>\", text) #Replace dates with tag\n",
    "    text = re.sub(r'''[a-z0-9._%+-]+\\@[a-z0-9.-]+[a-z0-9]\\.[a-z]{1,}''', \"<EMAIL>\", text)\n",
    "    #text = re.sub(r'''[0-9]+''', \"<NUM>\", text) #Replace numbers with tag\n",
    "    text = re.sub(r'''[.|,|!|?|\\'|\\''|\\\"|\\n|\\t|\\-|\\(|\\)]''', '', text)\n",
    "    text = re.sub(r'''^\\s+|\\s+$''', '', text) #Remove whitespaces at the end and start of string\n",
    "    text = re.sub(r'''[ ][ ]+|_''', \" \", text) #Remove multiple whitespace\n",
    "    if len(text) <= 0:\n",
    "        text = re.sub(r'''''', \"0\", text) #Remove multiple whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal dataset\n",
    "df = pd.read_csv(\"smallDomainDataNotEmbedded.csv\")\n",
    "\n",
    "# Epsilon 0,1 fair dataset\n",
    "#df = pd.read_csv(\"domain_data_with_identities_private_xtrain0,1.csv\")\n",
    "\n",
    "# Epsilon 0,01 fair dataset\n",
    "#df = pd.read_csv(\"domain_data_with_identities_private_xtrain0,1.csv\")\n",
    "\n",
    "#df['comment_text'] = df['comment_text'].apply(lambda text: CleanText(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating values for training_data\n",
    "training_data = df[df['split'] == 'train']\n",
    "#training_data = training_data.drop(training_data.query('toxicity==0').sample(frac=.85).index)\n",
    "\n",
    "# Getting test_data\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "# Getting validation_data\n",
    "validation_data = df[df['split'] == 'val']\n",
    "#validation_data = validation_data.drop(validation_data.query('toxicity==0').sample(frac=.85).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic samples training data:  710\n",
      "None-toxic samples training data:  710\n",
      "\n",
      "\n",
      "Toxic samples validation data:  360\n",
      "None-toxic samples validation data:  360\n",
      "\n",
      "\n",
      "Toxic samples test data:  360\n",
      "None-toxic samples test data:  360\n",
      "\n",
      "\n",
      "male: 322.0\n",
      "female: 276.0\n",
      "LGBTQ: 234.0\n",
      "christian: 214.0\n",
      "muslim: 236.0\n",
      "other_religion: 20.0\n",
      "black: 250.0\n",
      "white: 370.0\n"
     ]
    }
   ],
   "source": [
    "print('Toxic samples training data: ', sum(training_data['toxicity']))\n",
    "print('None-toxic samples training data: ', len(training_data['toxicity'])-sum(training_data['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Toxic samples validation data: ', sum(validation_data['toxicity']))\n",
    "print('None-toxic samples validation data: ', len(validation_data['toxicity'])-sum(validation_data['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Toxic samples test data: ', sum(test_data['toxicity']))\n",
    "print('None-toxic samples test data: ', len(test_data['toxicity'])-sum(test_data['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for col in training_data.columns[3:]:\n",
    "    print(col + \": \" + str(np.sum(training_data[col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>LGBTQ</th>\n",
       "      <th>christian</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Are you meaning to relate the quote from PT to...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>This guy had no business interacting with this...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>Yes, he is the poster child for white, heteros...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>What's wrong with expecting the normal degree ...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>Thanks man. but This ain't a democracy brother...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>More like you can't stand a man of value and d...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>good hombre....bad hombre....he was an illegal...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2357</th>\n",
       "      <td>Kind of like how if a male is the main charact...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>White men?\\nWhat?\\nAnd ignore shooters of othe...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>Doesn't he sort of remind you of Harper? Smart...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text  split  toxicity  \\\n",
       "51    Are you meaning to relate the quote from PT to...  train         0   \n",
       "1504  This guy had no business interacting with this...  train         0   \n",
       "1517  Yes, he is the poster child for white, heteros...  train         0   \n",
       "1534  What's wrong with expecting the normal degree ...  train         0   \n",
       "1537  Thanks man. but This ain't a democracy brother...  train         0   \n",
       "...                                                 ...    ...       ...   \n",
       "2355  More like you can't stand a man of value and d...  train         0   \n",
       "2356  good hombre....bad hombre....he was an illegal...  train         0   \n",
       "2357  Kind of like how if a male is the main charact...  train         0   \n",
       "2358  White men?\\nWhat?\\nAnd ignore shooters of othe...  train         0   \n",
       "2359  Doesn't he sort of remind you of Harper? Smart...  train         0   \n",
       "\n",
       "      male  female  LGBTQ  christian  muslim  other_religion  black  white  \n",
       "51     1.0     1.0    0.0        1.0     0.0             1.0    0.0    0.0  \n",
       "1504   1.0     1.0    0.0        1.0     0.0             0.0    0.0    0.0  \n",
       "1517   1.0     0.0    0.0        1.0     0.0             0.0    0.0    1.0  \n",
       "1534   1.0     1.0    0.0        1.0     0.0             0.0    0.0    1.0  \n",
       "1537   1.0     0.0    0.0        1.0     1.0             0.0    0.0    0.0  \n",
       "...    ...     ...    ...        ...     ...             ...    ...    ...  \n",
       "2355   1.0     0.0    0.0        0.0     0.0             0.0    0.0    0.0  \n",
       "2356   1.0     0.0    0.0        0.0     0.0             0.0    0.0    0.0  \n",
       "2357   1.0     0.0    0.0        0.0     0.0             0.0    0.0    0.0  \n",
       "2358   1.0     0.0    0.0        0.0     0.0             0.0    0.0    1.0  \n",
       "2359   1.0     0.0    0.0        0.0     0.0             0.0    0.0    0.0  \n",
       "\n",
       "[163 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.query('male==1 and toxicity==0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "currTrain = training_data['comment_text'].values.tolist()\n",
    "currTest = test_data['comment_text'].values.tolist()\n",
    "currVal = validation_data['comment_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (213 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)                     \n",
    "bertweet.eval()\n",
    "\n",
    "X_train = torch.zeros((1420, 768))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(currTrain)):\n",
    "        input_ids = torch.tensor([tokenizer.encode(currTrain[i])])\n",
    "        masks = [1]*len(input_ids[0])\n",
    "        masks = torch.tensor([masks])   \n",
    "        outputs = model(masks, input_ids)\n",
    "        X_train[i] = outputs[1][0]\n",
    "        \n",
    "    #for i in range(len(currTest)):\n",
    "    #    inputs = tokenizer([currTest[i]], return_tensors=\"pt\")\n",
    "    #    inputs = inputs.to('cuda')\n",
    "    #    outputs = model(**inputs)\n",
    "    #    X_test.append(outputs[1])\n",
    "    \n",
    "    #for i in range(len(currVal)):\n",
    "    #    inputs = tokenizer([currVal[i]], return_tensors=\"pt\")\n",
    "    #    inputs = inputs.to('cuda')\n",
    "    #    outputs = model(**inputs)\n",
    "    #    X_val.append(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does this fool ever work for Canadians.  I can't even look at his face anymore.  Resentment runs high when it comes to this clown. how much did this cost taxpayers....so he could like a prince.  Where is the Ethics officer with the Aga khan investigation?  Seriously, how long does that take. He has for Islam but not for his taxpaying Canadians.  Strange.\n",
      "Prove it Ms Khalid.  Launch the exact same motion but change out Islam and Islamophobia with Judaism and Anti-Semitism.  Or Christianity and Christopobia.  You are singling out your religion for protection from criticism.  We are not fooled by your lies.\n"
     ]
    }
   ],
   "source": [
    "print(currTrain[0])\n",
    "print(currTrain[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1420"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(currTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "### Set parameters for the model\n",
    "#torch.manual_seed(42) # set fixed random seed for reproducibility\n",
    "batch_size = 32\n",
    "epochs = 50000\n",
    "lr = 0.00002\n",
    "\n",
    "cuda = True # Set this if training on GPU\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Using \"+repr(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420\n",
      "1420\n"
     ]
    }
   ],
   "source": [
    "#transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Creating data loaders\n",
    "#X_train = np.array(training_data['comment_text'].values.tolist())\n",
    "Y_train = np.array(training_data['toxicity'].values.tolist())\n",
    "\n",
    "#X_test = np.array(test_data['comment_text'].values.tolist())\n",
    "Y_test = np.array(test_data['toxicity'].values.tolist())\n",
    "\n",
    "#X_val = np.array(validation_data['comment_text'].values.tolist())\n",
    "Y_val = np.array(validation_data['toxicity'].values.tolist())\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "\n",
    "prepare_trainloader = []\n",
    "for i in range(len(X_train)):\n",
    "    prepare_trainloader.append([X_train[i], Y_train[i]])\n",
    "    \n",
    "prepare_testloader = []\n",
    "for i in range(len(X_test)):\n",
    "    prepare_testloader.append([X_test[i], Y_test[i]])\n",
    "    \n",
    "prepare_validloader = []\n",
    "for i in range(len(X_val)):\n",
    "    prepare_validloader.append([X_val[i], Y_val[i]])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(prepare_trainloader, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(prepare_validloader, batch_size=len(prepare_validloader), shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(prepare_testloader, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckAccuracy(predictions, labels):\n",
    "        acc = 0.0\n",
    "        for i in range(len(predictions)):\n",
    "            if (predictions[i] == labels[i]):\n",
    "                acc += 1\n",
    "        return acc/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(768, 64), nn.ReLU(), nn.Dropout(p=0.8))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(64, 32), nn.ReLU())\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:  0.0\n",
      "F1_score:  0.0\n",
      "F1_score:  0.0\n",
      "F1_score:  0.0\n",
      "F1_score:  0.0\n",
      "F1_score:  0.0\n",
      "F1_score:  0.0\n",
      "F1_score:  0.0\n",
      "F1_score:  0.0\n",
      "F1_score:  0.027100271002710025\n",
      "F1_score:  0.07253886010362695\n",
      "F1_score:  0.00554016620498615\n",
      "F1_score:  0.1497584541062802\n",
      "F1_score:  0.18433179723502302\n",
      "F1_score:  0.16783216783216787\n",
      "F1_score:  0.25684210526315787\n",
      "F1_score:  0.19730941704035876\n",
      "F1_score:  0.379182156133829\n",
      "F1_score:  0.2803347280334728\n",
      "F1_score:  0.344294003868472\n",
      "F1_score:  0.43434343434343436\n",
      "F1_score:  0.3260437375745527\n",
      "F1_score:  0.4669887278582931\n",
      "F1_score:  0.4853168469860896\n",
      "F1_score:  0.3619047619047619\n",
      "F1_score:  0.5030120481927711\n",
      "F1_score:  0.483619344773791\n",
      "F1_score:  0.4827586206896552\n",
      "F1_score:  0.49319213313161875\n",
      "F1_score:  0.46984126984126984\n",
      "F1_score:  0.5065885797950219\n",
      "F1_score:  0.5176803394625177\n",
      "F1_score:  0.5276243093922652\n",
      "F1_score:  0.5222381635581061\n",
      "F1_score:  0.523121387283237\n",
      "F1_score:  0.510385756676558\n",
      "F1_score:  0.4615384615384615\n",
      "F1_score:  0.5278174037089871\n",
      "F1_score:  0.47900466562986005\n",
      "F1_score:  0.5233918128654971\n",
      "F1_score:  0.5310734463276837\n",
      "F1_score:  0.5255474452554745\n",
      "F1_score:  0.5226277372262774\n",
      "F1_score:  0.5295698924731184\n",
      "F1_score:  0.5279770444763271\n",
      "F1_score:  0.5277401894451961\n",
      "F1_score:  0.4961948249619483\n",
      "F1_score:  0.5082212257100149\n",
      "F1_score:  0.4984802431610942\n",
      "F1_score:  0.5327635327635328\n",
      "F1_score:  0.532608695652174\n",
      "F1_score:  0.5312934631432545\n",
      "F1_score:  0.5321888412017167\n",
      "F1_score:  0.5333333333333333\n",
      "F1_score:  0.5327868852459015\n",
      "F1_score:  0.5276967930029154\n",
      "F1_score:  0.5372340425531915\n",
      "F1_score:  0.5338865836791148\n",
      "F1_score:  0.5352112676056338\n",
      "F1_score:  0.5447368421052632\n",
      "F1_score:  0.5289017341040462\n",
      "F1_score:  0.5454545454545454\n",
      "F1_score:  0.5381414701803051\n",
      "F1_score:  0.5329670329670328\n",
      "F1_score:  0.5347885402455662\n",
      "F1_score:  0.5311653116531165\n",
      "F1_score:  0.5267727930535455\n",
      "F1_score:  0.5360544217687074\n",
      "F1_score:  0.5344352617079889\n",
      "F1_score:  0.5398601398601399\n",
      "F1_score:  0.533145275035261\n",
      "F1_score:  0.5383580080753702\n",
      "F1_score:  0.5426573426573427\n",
      "F1_score:  0.5383580080753702\n",
      "F1_score:  0.5379310344827586\n",
      "F1_score:  0.5397260273972603\n",
      "F1_score:  0.5342657342657342\n",
      "F1_score:  0.5360544217687074\n",
      "F1_score:  0.5377229080932784\n",
      "F1_score:  0.5045871559633027\n",
      "F1_score:  0.5357643758765779\n",
      "F1_score:  0.5524568393094289\n",
      "F1_score:  0.5260115606936416\n",
      "F1_score:  0.541049798115747\n",
      "F1_score:  0.5389876880984953\n",
      "F1_score:  0.5372714486638538\n",
      "F1_score:  0.5320056899004266\n",
      "F1_score:  0.5488621151271753\n",
      "F1_score:  0.5394190871369294\n",
      "F1_score:  0.544959128065395\n",
      "F1_score:  0.5189504373177842\n",
      "F1_score:  0.5293276108726752\n",
      "F1_score:  0.5378378378378378\n",
      "F1_score:  0.5384615384615384\n",
      "F1_score:  0.544959128065395\n",
      "F1_score:  0.5434782608695652\n",
      "F1_score:  0.5471698113207548\n",
      "F1_score:  0.5381414701803051\n",
      "F1_score:  0.5397260273972603\n",
      "F1_score:  0.5359116022099447\n",
      "F1_score:  0.5417236662106703\n",
      "F1_score:  0.5364511691884455\n",
      "F1_score:  0.5635648754914809\n",
      "F1_score:  0.5639686684073107\n",
      "F1_score:  0.5510752688172041\n",
      "F1_score:  0.5559947299077735\n",
      "F1_score:  0.5520974289580515\n",
      "F1_score:  0.5146198830409356\n",
      "F1_score:  0.5316455696202531\n",
      "F1_score:  0.5447042640990372\n",
      "F1_score:  0.5401662049861496\n",
      "F1_score:  0.5525606469002695\n",
      "F1_score:  0.5161290322580646\n",
      "F1_score:  0.5531914893617021\n",
      "F1_score:  0.5357643758765779\n",
      "F1_score:  0.5482093663911847\n",
      "F1_score:  0.5515394912985274\n",
      "F1_score:  0.5540540540540541\n",
      "F1_score:  0.5559947299077735\n",
      "F1_score:  0.5509641873278237\n",
      "F1_score:  0.5255681818181818\n",
      "F1_score:  0.5131964809384164\n",
      "F1_score:  0.5406162464985995\n",
      "F1_score:  0.5344585091420535\n",
      "F1_score:  0.547945205479452\n",
      "F1_score:  0.545945945945946\n",
      "F1_score:  0.5491251682368775\n",
      "F1_score:  0.5173410404624278\n",
      "F1_score:  0.5258620689655171\n",
      "F1_score:  0.5439330543933054\n",
      "F1_score:  0.551264980026631\n",
      "F1_score:  0.5639686684073107\n",
      "F1_score:  0.543956043956044\n",
      "F1_score:  0.5365168539325842\n",
      "F1_score:  0.5449515905947442\n",
      "F1_score:  0.5318246110325318\n",
      "F1_score:  0.5534759358288769\n",
      "F1_score:  0.5617685305591678\n",
      "F1_score:  0.5511596180081855\n",
      "F1_score:  0.5409153952843273\n",
      "F1_score:  0.5117647058823529\n",
      "F1_score:  0.5461956521739131\n",
      "F1_score:  0.5237410071942445\n",
      "F1_score:  0.5491803278688525\n",
      "F1_score:  0.5684485006518905\n",
      "F1_score:  0.5528455284552845\n",
      "F1_score:  0.5255681818181818\n",
      "F1_score:  0.527065527065527\n",
      "F1_score:  0.5363128491620112\n",
      "F1_score:  0.5643693107932379\n",
      "F1_score:  0.5187319884726225\n",
      "F1_score:  0.5297450424929179\n",
      "F1_score:  0.5543478260869564\n",
      "F1_score:  0.5414364640883977\n",
      "F1_score:  0.5499316005471957\n",
      "F1_score:  0.5338983050847459\n",
      "F1_score:  0.5582010582010581\n",
      "F1_score:  0.5100864553314121\n",
      "F1_score:  0.5388888888888889\n",
      "F1_score:  0.5096296296296297\n",
      "F1_score:  0.5271966527196653\n",
      "F1_score:  0.5611702127659575\n",
      "F1_score:  0.5151079136690647\n",
      "F1_score:  0.5449515905947442\n",
      "F1_score:  0.5597897503285151\n",
      "F1_score:  0.5491803278688525\n",
      "F1_score:  0.5609436435124509\n",
      "F1_score:  0.5449515905947442\n",
      "F1_score:  0.5116959064327485\n",
      "F1_score:  0.5542168674698795\n",
      "F1_score:  0.5286713286713286\n",
      "F1_score:  0.5340751043115438\n",
      "F1_score:  0.5348189415041782\n",
      "F1_score:  0.5164992826398852\n",
      "F1_score:  0.5493910690121787\n",
      "F1_score:  0.547945205479452\n",
      "F1_score:  0.5340751043115438\n",
      "F1_score:  0.5699481865284973\n",
      "F1_score:  0.5240793201133145\n",
      "F1_score:  0.543956043956044\n",
      "F1_score:  0.5476510067114093\n",
      "F1_score:  0.5669700910273082\n",
      "F1_score:  0.5299860529986052\n",
      "F1_score:  0.5065885797950219\n",
      "F1_score:  0.524822695035461\n",
      "F1_score:  0.5394190871369294\n",
      "F1_score:  0.5095168374816983\n",
      "F1_score:  0.5515394912985274\n",
      "F1_score:  0.54320987654321\n",
      "F1_score:  0.5429740791268759\n",
      "F1_score:  0.527065527065527\n",
      "F1_score:  0.5373961218836565\n",
      "F1_score:  0.5725490196078431\n",
      "F1_score:  0.5382513661202185\n",
      "F1_score:  0.5102040816326531\n",
      "F1_score:  0.5364511691884455\n",
      "F1_score:  0.5261669024045262\n",
      "F1_score:  0.5364511691884455\n",
      "F1_score:  0.5739570164348924\n",
      "F1_score:  0.5524568393094289\n",
      "F1_score:  0.5316804407713498\n",
      "F1_score:  0.49022556390977445\n",
      "F1_score:  0.5434782608695652\n",
      "F1_score:  0.5270457697642164\n",
      "F1_score:  0.5286713286713286\n",
      "F1_score:  0.5217391304347826\n",
      "F1_score:  0.5623342175066313\n",
      "F1_score:  0.5216138328530259\n",
      "F1_score:  0.5739795918367346\n",
      "F1_score:  0.5291607396870555\n",
      "F1_score:  0.5329670329670328\n",
      "F1_score:  0.5307262569832403\n",
      "F1_score:  0.5538057742782151\n",
      "F1_score:  0.5044510385756676\n",
      "F1_score:  0.544973544973545\n",
      "F1_score:  0.5521796565389697\n",
      "F1_score:  0.5359116022099447\n",
      "F1_score:  0.527065527065527\n",
      "F1_score:  0.5327868852459015\n",
      "F1_score:  0.5317139001349529\n",
      "F1_score:  0.5357142857142857\n",
      "F1_score:  0.5389876880984953\n",
      "F1_score:  0.5276595744680852\n",
      "F1_score:  0.5344352617079889\n",
      "F1_score:  0.540983606557377\n",
      "F1_score:  0.5233380480905233\n",
      "F1_score:  0.5325936199722607\n",
      "F1_score:  0.5209840810419682\n",
      "F1_score:  0.5371900826446282\n",
      "F1_score:  0.5310734463276837\n",
      "F1_score:  0.5193687230989956\n",
      "F1_score:  0.52046783625731\n",
      "F1_score:  0.5468956406869221\n",
      "F1_score:  0.5060606060606061\n",
      "F1_score:  0.5203488372093023\n",
      "F1_score:  0.5340599455040871\n",
      "F1_score:  0.52046783625731\n",
      "F1_score:  0.5186246418338109\n",
      "F1_score:  0.5310734463276837\n",
      "F1_score:  0.5355648535564853\n",
      "F1_score:  0.5342465753424657\n",
      "F1_score:  0.5335157318741448\n",
      "F1_score:  0.5242165242165242\n",
      "F1_score:  0.5294117647058824\n",
      "F1_score:  0.5283540802213001\n",
      "F1_score:  0.5290858725761772\n",
      "F1_score:  0.5286713286713286\n",
      "F1_score:  0.5219858156028369\n",
      "F1_score:  0.5244956772334294\n",
      "F1_score:  0.5284671532846715\n",
      "F1_score:  0.5375170532060027\n",
      "F1_score:  0.561038961038961\n",
      "F1_score:  0.5244444444444445\n",
      "F1_score:  0.5437415881561238\n",
      "F1_score:  0.5279770444763271\n",
      "F1_score:  0.5196629213483146\n",
      "F1_score:  0.5289256198347108\n",
      "F1_score:  0.5337001375515819\n",
      "F1_score:  0.5261669024045262\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "# Setting up model parameters\n",
    "model = Net().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "loss_function = nn.BCELoss()\n",
    "#loss_function = nn.MSELoss()\n",
    "#loss_function = nn.NLLLoss()\n",
    "\n",
    "# Initialising early stopping criterias\n",
    "early_stopping = 50\n",
    "notImproved = 0\n",
    "bestLoss = None\n",
    "bestModel = None\n",
    "\n",
    "trainArr = []\n",
    "valArr = []\n",
    "\n",
    "bestf1 = 0\n",
    "bestEpoch = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        \n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize            \n",
    "        outputs = model(inputs).squeeze()\n",
    "        \n",
    "        \n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "    #    print('====> Epoch: {} Average training loss: {:.6f}'.format(epoch, train_loss))\n",
    "    \n",
    "    trainArr.append(train_loss)\n",
    "    \n",
    "    valid_loss = 0\n",
    "    labs = []\n",
    "    preds = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():        \n",
    "        for batch_idx, data in enumerate(valid_loader):\n",
    "            # get the input\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            inputs = inputs.to(device).float()\n",
    "            #labels = labels.to(device).long()\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            \n",
    "            labs.extend(labels)\n",
    "            preds.extend(torch.round(outputs))\n",
    "            #preds.extend(outputs.argmax(axis=1))\n",
    "            \n",
    "            valid_loss += loss_function(outputs, labels).item()\n",
    "    \n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    \n",
    "    valArr.append(valid_loss)\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "        #print('====> Validation set loss: {:.6f}'.format(valid_loss))\n",
    "    \n",
    "        #print(\"Accuracy on Validation set: \", CheckAccuracy(labs, preds))\n",
    "    \n",
    "    f1 = f1_score(torch.Tensor(labs).numpy(), torch.Tensor(preds).numpy(), zero_division=1)\n",
    "    print(\"F1_score: \", f1)\n",
    "    \n",
    "    if f1 > bestf1:\n",
    "        bestModel = model\n",
    "        bestf1 = f1\n",
    "        notImproved = 0\n",
    "        bestEpoch = epoch\n",
    "    else:\n",
    "        notImproved +=1\n",
    "        \n",
    "    if notImproved >= early_stopping:\n",
    "        break\n",
    "        \n",
    "    \n",
    "    # Initialising params for early stopping\n",
    "    #if bestLoss == None:\n",
    "    #    bestLoss = valid_loss\n",
    "        \n",
    "    # Checks for early stopping\n",
    "    #if f1 < bestf1:\n",
    "    #    bestLoss = valid_loss\n",
    "        \n",
    "    # Converges if the training has not improved for a certain amount of iterations\n",
    "    \n",
    "\n",
    "model = bestModel\n",
    "\n",
    "print(bestEpoch)\n",
    "\n",
    "#torch.save(model, 'bestModelFullyConnectedNetwork0,1Fair.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(np.arange(0,len(trainArr)), trainArr, color='r', label='Training loss')\n",
    "plt.scatter(np.arange(0,len(valArr)), valArr, color='g', label='Validation loss')\n",
    "plt.title(\"Training loss vs Validation loss epsilon 0,1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_Scores(preds, labels):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for i in range(len(labs)):\n",
    "        if labels[i]==1 and preds[i]==1:\n",
    "            true_positives += 1\n",
    "        if labels[i]==0 and preds[i]==0:\n",
    "            true_negatives += 1\n",
    "        if labels[i]==0 and preds[i]==1:\n",
    "            false_positives += 1\n",
    "        if labels[i]==1 and preds[i]==0:\n",
    "            false_negatives += 1\n",
    "    print(\"true_positives\", true_positives)\n",
    "    print(\"true_negatives\", true_negatives)\n",
    "    print(\"false_positives\", false_positives)\n",
    "    print(\"false_negatives\", false_negatives)\n",
    "    \n",
    "    return true_positives, true_negatives, false_positives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.5680555555555555\n",
      "Accuracy on validation set:  0.5513888888888889\n",
      "Accuracy on train set:  0.5786713286713286\n",
      "F1_score:  0.5725434551259312\n",
      "true_positives 807\n",
      "true_negatives 848\n",
      "false_positives 582\n",
      "false_negatives 623\n"
     ]
    }
   ],
   "source": [
    "labs = []\n",
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        \n",
    "        labs.extend(labels)\n",
    "        preds.extend(torch.round(outputs))\n",
    "        \n",
    "print(\"Accuracy on test set: \", CheckAccuracy(labs, preds))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "    for batch_idx, data in enumerate(valid_loader):\n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        \n",
    "        labs.extend(labels)\n",
    "        preds.extend(torch.round(outputs))\n",
    "        \n",
    "print(\"Accuracy on validation set: \", CheckAccuracy(labs, preds))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        \n",
    "        labs.extend(labels)\n",
    "        preds.extend(torch.round(outputs))\n",
    "        \n",
    "print(\"Accuracy on train set: \", CheckAccuracy(labs, preds))\n",
    "\n",
    "f1 = f1_score(torch.Tensor(labs).numpy(), torch.Tensor(preds).numpy(), zero_division=1)\n",
    "print(\"F1_score: \", f1)\n",
    "\n",
    "true_positives, true_negatives, false_positives, false_negatives = F1_Scores(preds, labs)\n",
    "\n",
    "#print(\"Toxic accuracy: \", true_positives/5640)\n",
    "#print(\"Non-toxic accuracy: \", true_negatives/42235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "F1_score:  0.29494949494949496\n",
      "0.5152777777777777\n",
      "\n",
      "\n",
      "0.001\n",
      "F1_score:  0.6327568667344863\n",
      "0.4986111111111111\n",
      "\n",
      "\n",
      "0.01\n",
      "F1_score:  0.1763341067285383\n",
      "0.5069444444444444\n",
      "\n",
      "\n",
      "0.1\n",
      "F1_score:  0.431496062992126\n",
      "0.4986111111111111\n",
      "\n",
      "\n",
      "1\n",
      "F1_score:  0.5142857142857143\n",
      "0.5041666666666667\n",
      "\n",
      "\n",
      "10\n",
      "F1_score:  0.5066666666666667\n",
      "0.5375\n",
      "\n",
      "\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:  0.5127516778523491\n",
      "0.49583333333333335\n",
      "\n",
      "\n",
      "40\n",
      "F1_score:  0.529960053262317\n",
      "0.5097222222222222\n",
      "\n",
      "\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:394: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:394: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:  0.5097493036211698\n",
      "0.5111111111111111\n",
      "\n",
      "\n",
      "60\n",
      "F1_score:  0.5623268698060941\n",
      "0.5611111111111111\n",
      "\n",
      "\n",
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:394: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:394: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:  0.5058139534883721\n",
      "0.5277777777777778\n",
      "\n",
      "\n",
      "80\n",
      "F1_score:  0.5314285714285715\n",
      "0.5444444444444444\n",
      "\n",
      "\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:394: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:  0.5244565217391304\n",
      "0.5138888888888888\n",
      "\n",
      "\n",
      "1000\n",
      "F1_score:  0.5439330543933054\n",
      "0.5458333333333333\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:394: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:221: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n",
      "C:\\Users\\frede\\anaconda3\\lib\\site-packages\\diffprivlib\\models\\logistic_regression.py:394: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from diffprivlib.models import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "epsilons = [0.0001, 0.001, 0.01, 0.1, 1, 10, 30, 40, 50, 60, 70, 80, 90, 1000]\n",
    "\n",
    "\n",
    "\n",
    "for eps in epsilons:\n",
    "    print(eps)\n",
    "    clf = LogisticRegression(epsilon=eps)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    f1 = f1_score(predictions, Y_test, zero_division=1)\n",
    "    \n",
    "    print(\"F1_score: \", f1)\n",
    "    \n",
    "    print(clf.score(X_test, Y_test))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #F1_Scores(predictions, Y_test)\n",
    "\n",
    "#predictions = np.array(predictions).reshape(len(predictions), 1)\n",
    "#Y_test = np.array(Y_test).reshape(len(Y_test), 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
