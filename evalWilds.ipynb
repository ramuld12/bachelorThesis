{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sized-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sapphire-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "df = pd.read_csv('CSVFiles/all_data_with_identities_50000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "valuable-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataframe to only contain the demographic groups we want\n",
    "newDF = df.loc[:, [\"split\", \"toxicity\", \"male\", \"female\", \"LGBTQ\", \"christian\", \"muslim\", \"other_religions\", \"black\", \"white\"]]\n",
    "\n",
    "# Drop all training data\n",
    "newDF = newDF.drop(newDF[newDF['split'] == 'train'].index)\n",
    "\n",
    "newDF['male'] = newDF['male'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['female'] = newDF['female'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['LGBTQ'] = newDF['LGBTQ'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['christian'] = newDF['christian'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['muslim'] = newDF['muslim'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['black'] = newDF['black'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['white'] = newDF['white'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['other_religions'] = newDF['other_religions'].apply(lambda x: np.round(x >= 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-intermediate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-furniture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "surprising-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting test_data\n",
    "test_data = newDF[newDF['split'] == 'test']\n",
    "\n",
    "# Getting validation_data\n",
    "validation_data = newDF[newDF['split'] == 'val']\n",
    "\n",
    "# Creating data loaders\n",
    "test_labels = np.array(test_data['toxicity'].values.tolist())\n",
    "val_labels = np.array(validation_data['toxicity'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surgical-arctic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-maker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-accordance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "engaging-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPositivesAndNegativesGroup(group, preds, labels):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i]==1 and preds[i]==1 and group[i] == 1:\n",
    "            true_positives += 1\n",
    "        if labels[i]==0 and preds[i]==0 and group[i] == 1:\n",
    "            true_negatives += 1\n",
    "        if labels[i]==0 and preds[i]==1 and group[i] == 1:\n",
    "            false_positives += 1\n",
    "        if labels[i]==1 and preds[i]==0 and group[i] == 1:\n",
    "            false_negatives += 1\n",
    "    \n",
    "    return true_positives, true_negatives, false_positives, false_negatives\n",
    "\n",
    "def CheckAccuracy(predictions, labels):\n",
    "        acc = 0.0\n",
    "        for i in range(len(predictions)):\n",
    "            if (predictions[i] == labels[i]):\n",
    "                acc += 1\n",
    "        return acc/len(predictions)\n",
    "    \n",
    "def checkAccDemoGroup(group, preds, toxicArray):\n",
    "    assert len(group) == len(preds) and len(group) == len(toxicArray)\n",
    "    true_positives, true_negatives, false_positives, false_negatives = getPositivesAndNegativesGroup(group, preds, toxicArray)\n",
    "    total_toxic = true_positives+false_negatives\n",
    "    total_nonToxic = true_negatives+false_positives\n",
    "    acc_toxic = true_positives/total_toxic\n",
    "    acc_nonToxic = true_negatives/total_nonToxic\n",
    "    return round(acc_toxic, 3), round(acc_nonToxic, 3), total_toxic, total_nonToxic\n",
    "\n",
    "def checkF1DemoGroup(group, preds, toxicArray):\n",
    "    assert len(group) == len(preds) and len(group) == len(toxicArray)\n",
    "    true_positives, true_negatives, false_positives, false_negatives = getPositivesAndNegativesGroup(group, preds, toxicArray)\n",
    "    total_group = true_positives+false_negatives+true_negatives+false_positives\n",
    "    precision = true_positives/(true_positives + false_positives)\n",
    "    recall = true_positives/(true_positives + false_negatives)\n",
    "    \n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    return round(f1_score, 3), total_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-china",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "independent-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestAndValPreds(seed, pathPrefix):\n",
    "    test_preds = []\n",
    "    val_preds = []\n",
    "    test_path = pathPrefix+'test_seed-'+ str(seed) + '_epoch-best_pred.csv'\n",
    "    val_path = pathPrefix+'val_seed-'+ str(seed) + '_epoch-best_pred.csv'\n",
    "    \n",
    "    with open(test_path, 'r') as read_obj:\n",
    "        csv_reader = csv.reader(read_obj)\n",
    "        for row in csv_reader:\n",
    "            test_preds.append(int(row[0]))\n",
    "\n",
    "    with open(val_path, 'r') as read_obj:\n",
    "        csv_reader = csv.reader(read_obj)\n",
    "        for row in csv_reader:\n",
    "            val_preds.append(int(row[0]))\n",
    "\n",
    "    test_preds = np.array(test_preds)\n",
    "    val_preds = np.array(val_preds)\n",
    "    \n",
    "    return test_preds, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-facility",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-raleigh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-confirmation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-institution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-retailer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-writer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affecting-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEvalResults(pathPrefix):\n",
    "    avg_accuracies = []\n",
    "    avg_toxic_accuracies = []\n",
    "    avg_nonToxic_accuracies = []\n",
    "    global_worst_acc = 1\n",
    "    global_worst_group_seed_acc = ''\n",
    "    avg_f1Scores = []\n",
    "    global_worst_f1 = 1\n",
    "    global_worst_group_seed_f1 = ''\n",
    "    toxicArray = np.array(test_data['toxicity'].values.tolist())\n",
    "    #seeds = [1,5,10,20,30,40,50,60,70,80,90]\n",
    "    #seeds = [100,200,300,400,500,600,700,800,900,1000]\n",
    "    for i in range(1,11):\n",
    "        print('\\n---------------- Seed ' + str(i) + '---------------\\n')\n",
    "        test_preds, val_preds = loadTestAndValPreds(i, pathPrefix)\n",
    "        worst_acc = 1\n",
    "        worst_f1 = 1\n",
    "        worst_group_acc = ''\n",
    "        worst_group_f1 = ''\n",
    "        toxic_accuracies = []\n",
    "        nonToxic_accuracies = []\n",
    "        overall_toxic = 0\n",
    "        overall_nonToxic = 0\n",
    "\n",
    "        # Go through all but the first two columns and check accuracy\n",
    "        for curCol in test_data.iloc[:,2:]:\n",
    "            curColArray = np.array(test_data[curCol].values.tolist())\n",
    "            cur_f1, total_group = checkF1DemoGroup(curColArray, test_preds, toxicArray)\n",
    "            acc_toxic, acc_nonToxic, total_toxic, total_nonToxic = checkAccDemoGroup(curColArray, test_preds, toxicArray)\n",
    "            if curCol == 'christian':\n",
    "                print(f'{curCol} \\tacc on non_toxic: {acc_nonToxic:.3f} ({total_nonToxic}) \\tacc on toxic: {acc_toxic:.3f} ({total_toxic})\\tf1_score: {cur_f1:.3f} ({total_group})')\n",
    "            elif curCol == 'other_religions':\n",
    "                print(f'{curCol} acc on non_toxic: {acc_nonToxic:.3f} ({total_nonToxic})  \\tacc on toxic: {acc_toxic:.3f} ({total_toxic})\\tf1_score: {cur_f1:.3f} ({total_group})')\n",
    "            else:\n",
    "                print(f'{curCol} \\t\\tacc on non_toxic: {acc_nonToxic:.3f} ({total_nonToxic})  \\tacc on toxic: {acc_toxic:.3f} ({total_toxic})\\tf1_score: {cur_f1:.3f} ({total_group})')\n",
    "            if worst_acc > acc_toxic:\n",
    "                worst_acc = acc_toxic\n",
    "                worst_group_acc = curCol\n",
    "            elif worst_acc > acc_nonToxic:\n",
    "                worst_acc = acc_nonToxic\n",
    "                worst_group_acc = curCol\n",
    "            if worst_f1 > cur_f1:\n",
    "                worst_f1 = cur_f1\n",
    "                worst_group_f1 = curCol\n",
    "            if global_worst_f1 > worst_f1:\n",
    "                    global_worst_f1 = worst_f1\n",
    "                    global_worst_group_seed_f1 = f' for demographic group {curCol} in seed {i}'\n",
    "            if global_worst_acc > worst_acc:\n",
    "                    global_worst_acc = worst_acc\n",
    "                    global_worst_group_seed_acc = f' for demographic group {curCol} in seed {i}'\n",
    "            toxic_accuracies.append(acc_toxic*total_toxic)\n",
    "            nonToxic_accuracies.append(acc_nonToxic*total_nonToxic)\n",
    "            overall_toxic += total_toxic\n",
    "            overall_nonToxic += total_nonToxic\n",
    "        avgSeedAcc = CheckAccuracy(test_preds, toxicArray)\n",
    "        avgSeedF1 = f1_score(toxicArray, test_preds, zero_division=1)\n",
    "        avgSeedAccToxic = sum(toxic_accuracies)/overall_toxic\n",
    "        avgSeedAccNonToxic = sum(nonToxic_accuracies)/overall_nonToxic\n",
    "        print(f'Average accuracy on non toxic for seed is: {avgSeedAccNonToxic:.3f}')\n",
    "        print(f'Average accuracy on toxic for seed is: {avgSeedAccToxic:.3f}')\n",
    "        print(f'Average accuracy for seed is: {avgSeedAcc:.3f}')\n",
    "        print(f'Worst accuracy is {worst_acc:.3f} for demographic group {worst_group_acc}')\n",
    "        print(f'Average f1 for seed is: {avgSeedF1:.3f}')\n",
    "        print(f'Worst f1 is {worst_f1:.3f} for demographic group {worst_group_f1}')\n",
    "        avg_accuracies.append(avgSeedAcc)\n",
    "        avg_f1Scores.append(avgSeedF1)\n",
    "        avg_nonToxic_accuracies.append(avgSeedAccNonToxic)\n",
    "        avg_toxic_accuracies.append(avgSeedAccToxic)\n",
    "    overall_toxic_acc = sum(avg_toxic_accuracies)/len(avg_toxic_accuracies)\n",
    "    overall_nonToxic_acc = sum(avg_nonToxic_accuracies)/len(avg_nonToxic_accuracies)\n",
    "    overall_avg_acc = sum(avg_accuracies)/len(avg_accuracies)\n",
    "    overall_avg_f1 = sum(avg_f1Scores)/len(avg_f1Scores)\n",
    "\n",
    "    print('\\n---------------- Overall evaluation---------------\\n')\n",
    "    print(f'Overall non toxic acc. is: {overall_nonToxic_acc:.3f}')\n",
    "    print(f'Overall toxic acc. is: {overall_toxic_acc:.3f}')\n",
    "    print(f'Overall avg. acc. is: {overall_avg_acc:.3f}')\n",
    "    print(f'Overall worst acc. is: {global_worst_acc:.3f}{global_worst_group_seed_acc}')\n",
    "    print(f'Overall avg. f1. is: {overall_avg_f1:.3f}')\n",
    "    print(f'Overall worst f1. is: {global_worst_f1:.3f}{global_worst_group_seed_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "later-survey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- Seed 1---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.930 (86)  \tacc on toxic: 0.626 (254)\tf1_score: 0.759 (340)\n",
      "female \t\tacc on non_toxic: 0.910 (122)  \tacc on toxic: 0.600 (280)\tf1_score: 0.732 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 1.000 (7)  \tacc on toxic: 0.698 (162)\tf1_score: 0.822 (169)\n",
      "christian \tacc on non_toxic: 0.962 (78) \tacc on toxic: 0.623 (159)\tf1_score: 0.759 (237)\n",
      "muslim \t\tacc on non_toxic: 0.892 (37)  \tacc on toxic: 0.737 (190)\tf1_score: 0.838 (227)\n",
      "other_religions acc on non_toxic: 0.824 (34)  \tacc on toxic: 0.662 (68)\tf1_score: 0.756 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.703 (175)\tf1_score: 0.820 (180)\n",
      "white \t\tacc on non_toxic: 0.667 (21)  \tacc on toxic: 0.655 (267)\tf1_score: 0.780 (288)\n",
      "Average accuracy on non toxic for seed is: 0.900\n",
      "Average accuracy on toxic for seed is: 0.657\n",
      "Average accuracy for seed is: 0.786\n",
      "Worst accuracy is 0.600 for demographic group female\n",
      "Average f1 for seed is: 0.740\n",
      "Worst f1 is 0.732 for demographic group female\n",
      "\n",
      "---------------- Seed 2---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.930 (86)  \tacc on toxic: 0.646 (254)\tf1_score: 0.774 (340)\n",
      "female \t\tacc on non_toxic: 0.902 (122)  \tacc on toxic: 0.646 (280)\tf1_score: 0.765 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 1.000 (7)  \tacc on toxic: 0.698 (162)\tf1_score: 0.822 (169)\n",
      "christian \tacc on non_toxic: 0.923 (78) \tacc on toxic: 0.673 (159)\tf1_score: 0.787 (237)\n",
      "muslim \t\tacc on non_toxic: 0.838 (37)  \tacc on toxic: 0.768 (190)\tf1_score: 0.854 (227)\n",
      "other_religions acc on non_toxic: 0.794 (34)  \tacc on toxic: 0.676 (68)\tf1_score: 0.760 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.697 (175)\tf1_score: 0.816 (180)\n",
      "white \t\tacc on non_toxic: 0.762 (21)  \tacc on toxic: 0.648 (267)\tf1_score: 0.778 (288)\n",
      "Average accuracy on non toxic for seed is: 0.887\n",
      "Average accuracy on toxic for seed is: 0.676\n",
      "Average accuracy for seed is: 0.800\n",
      "Worst accuracy is 0.600 for demographic group black\n",
      "Average f1 for seed is: 0.763\n",
      "Worst f1 is 0.760 for demographic group other_religions\n",
      "\n",
      "---------------- Seed 3---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.942 (86)  \tacc on toxic: 0.646 (254)\tf1_score: 0.775 (340)\n",
      "female \t\tacc on non_toxic: 0.902 (122)  \tacc on toxic: 0.657 (280)\tf1_score: 0.773 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 1.000 (7)  \tacc on toxic: 0.722 (162)\tf1_score: 0.839 (169)\n",
      "christian \tacc on non_toxic: 0.949 (78) \tacc on toxic: 0.629 (159)\tf1_score: 0.760 (237)\n",
      "muslim \t\tacc on non_toxic: 0.838 (37)  \tacc on toxic: 0.774 (190)\tf1_score: 0.857 (227)\n",
      "other_religions acc on non_toxic: 0.794 (34)  \tacc on toxic: 0.676 (68)\tf1_score: 0.760 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.726 (175)\tf1_score: 0.836 (180)\n",
      "white \t\tacc on non_toxic: 0.857 (21)  \tacc on toxic: 0.618 (267)\tf1_score: 0.759 (288)\n",
      "Average accuracy on non toxic for seed is: 0.900\n",
      "Average accuracy on toxic for seed is: 0.675\n",
      "Average accuracy for seed is: 0.796\n",
      "Worst accuracy is 0.600 for demographic group black\n",
      "Average f1 for seed is: 0.754\n",
      "Worst f1 is 0.759 for demographic group white\n",
      "\n",
      "---------------- Seed 4---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.907 (86)  \tacc on toxic: 0.693 (254)\tf1_score: 0.804 (340)\n",
      "female \t\tacc on non_toxic: 0.877 (122)  \tacc on toxic: 0.682 (280)\tf1_score: 0.786 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 1.000 (7)  \tacc on toxic: 0.784 (162)\tf1_score: 0.879 (169)\n",
      "christian \tacc on non_toxic: 0.962 (78) \tacc on toxic: 0.667 (159)\tf1_score: 0.791 (237)\n",
      "muslim \t\tacc on non_toxic: 0.838 (37)  \tacc on toxic: 0.763 (190)\tf1_score: 0.850 (227)\n",
      "other_religions acc on non_toxic: 0.794 (34)  \tacc on toxic: 0.706 (68)\tf1_score: 0.780 (102)\n",
      "black \t\tacc on non_toxic: 0.400 (5)  \tacc on toxic: 0.800 (175)\tf1_score: 0.881 (180)\n",
      "white \t\tacc on non_toxic: 0.571 (21)  \tacc on toxic: 0.693 (267)\tf1_score: 0.803 (288)\n",
      "Average accuracy on non toxic for seed is: 0.869\n",
      "Average accuracy on toxic for seed is: 0.719\n",
      "Average accuracy for seed is: 0.808\n",
      "Worst accuracy is 0.400 for demographic group black\n",
      "Average f1 for seed is: 0.774\n",
      "Worst f1 is 0.780 for demographic group other_religions\n",
      "\n",
      "---------------- Seed 5---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.907 (86)  \tacc on toxic: 0.626 (254)\tf1_score: 0.755 (340)\n",
      "female \t\tacc on non_toxic: 0.877 (122)  \tacc on toxic: 0.636 (280)\tf1_score: 0.753 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 0.857 (7)  \tacc on toxic: 0.735 (162)\tf1_score: 0.844 (169)\n",
      "christian \tacc on non_toxic: 0.936 (78) \tacc on toxic: 0.642 (159)\tf1_score: 0.767 (237)\n",
      "muslim \t\tacc on non_toxic: 0.838 (37)  \tacc on toxic: 0.705 (190)\tf1_score: 0.812 (227)\n",
      "other_religions acc on non_toxic: 0.794 (34)  \tacc on toxic: 0.647 (68)\tf1_score: 0.739 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.663 (175)\tf1_score: 0.792 (180)\n",
      "white \t\tacc on non_toxic: 0.762 (21)  \tacc on toxic: 0.566 (267)\tf1_score: 0.714 (288)\n",
      "Average accuracy on non toxic for seed is: 0.874\n",
      "Average accuracy on toxic for seed is: 0.645\n",
      "Average accuracy for seed is: 0.787\n",
      "Worst accuracy is 0.566 for demographic group white\n",
      "Average f1 for seed is: 0.743\n",
      "Worst f1 is 0.714 for demographic group white\n",
      "\n",
      "---------------- Seed 6---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.930 (86)  \tacc on toxic: 0.634 (254)\tf1_score: 0.765 (340)\n",
      "female \t\tacc on non_toxic: 0.885 (122)  \tacc on toxic: 0.639 (280)\tf1_score: 0.757 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 1.000 (7)  \tacc on toxic: 0.778 (162)\tf1_score: 0.875 (169)\n",
      "christian \tacc on non_toxic: 0.936 (78) \tacc on toxic: 0.629 (159)\tf1_score: 0.758 (237)\n",
      "muslim \t\tacc on non_toxic: 0.865 (37)  \tacc on toxic: 0.737 (190)\tf1_score: 0.836 (227)\n",
      "other_religions acc on non_toxic: 0.794 (34)  \tacc on toxic: 0.632 (68)\tf1_score: 0.729 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.731 (175)\tf1_score: 0.839 (180)\n",
      "white \t\tacc on non_toxic: 0.667 (21)  \tacc on toxic: 0.622 (267)\tf1_score: 0.755 (288)\n",
      "Average accuracy on non toxic for seed is: 0.882\n",
      "Average accuracy on toxic for seed is: 0.671\n",
      "Average accuracy for seed is: 0.792\n",
      "Worst accuracy is 0.600 for demographic group black\n",
      "Average f1 for seed is: 0.747\n",
      "Worst f1 is 0.729 for demographic group other_religions\n",
      "\n",
      "---------------- Seed 7---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.930 (86)  \tacc on toxic: 0.657 (254)\tf1_score: 0.782 (340)\n",
      "female \t\tacc on non_toxic: 0.902 (122)  \tacc on toxic: 0.679 (280)\tf1_score: 0.788 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 0.857 (7)  \tacc on toxic: 0.722 (162)\tf1_score: 0.836 (169)\n",
      "christian \tacc on non_toxic: 0.910 (78) \tacc on toxic: 0.654 (159)\tf1_score: 0.770 (237)\n",
      "muslim \t\tacc on non_toxic: 0.838 (37)  \tacc on toxic: 0.789 (190)\tf1_score: 0.867 (227)\n",
      "other_religions acc on non_toxic: 0.765 (34)  \tacc on toxic: 0.632 (68)\tf1_score: 0.723 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.714 (175)\tf1_score: 0.828 (180)\n",
      "white \t\tacc on non_toxic: 0.762 (21)  \tacc on toxic: 0.622 (267)\tf1_score: 0.758 (288)\n",
      "Average accuracy on non toxic for seed is: 0.880\n",
      "Average accuracy on toxic for seed is: 0.683\n",
      "Average accuracy for seed is: 0.798\n",
      "Worst accuracy is 0.600 for demographic group black\n",
      "Average f1 for seed is: 0.757\n",
      "Worst f1 is 0.723 for demographic group other_religions\n",
      "\n",
      "---------------- Seed 8---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.919 (86)  \tacc on toxic: 0.634 (254)\tf1_score: 0.763 (340)\n",
      "female \t\tacc on non_toxic: 0.877 (122)  \tacc on toxic: 0.596 (280)\tf1_score: 0.723 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 1.000 (7)  \tacc on toxic: 0.636 (162)\tf1_score: 0.777 (169)\n",
      "christian \tacc on non_toxic: 0.936 (78) \tacc on toxic: 0.591 (159)\tf1_score: 0.729 (237)\n",
      "muslim \t\tacc on non_toxic: 0.838 (37)  \tacc on toxic: 0.674 (190)\tf1_score: 0.790 (227)\n",
      "other_religions acc on non_toxic: 0.765 (34)  \tacc on toxic: 0.618 (68)\tf1_score: 0.712 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.691 (175)\tf1_score: 0.812 (180)\n",
      "white \t\tacc on non_toxic: 0.810 (21)  \tacc on toxic: 0.637 (267)\tf1_score: 0.771 (288)\n",
      "Average accuracy on non toxic for seed is: 0.880\n",
      "Average accuracy on toxic for seed is: 0.634\n",
      "Average accuracy for seed is: 0.789\n",
      "Worst accuracy is 0.591 for demographic group christian\n",
      "Average f1 for seed is: 0.745\n",
      "Worst f1 is 0.712 for demographic group other_religions\n",
      "\n",
      "---------------- Seed 9---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.930 (86)  \tacc on toxic: 0.626 (254)\tf1_score: 0.759 (340)\n",
      "female \t\tacc on non_toxic: 0.902 (122)  \tacc on toxic: 0.607 (280)\tf1_score: 0.736 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 1.000 (7)  \tacc on toxic: 0.710 (162)\tf1_score: 0.830 (169)\n",
      "christian \tacc on non_toxic: 0.987 (78) \tacc on toxic: 0.616 (159)\tf1_score: 0.760 (237)\n",
      "muslim \t\tacc on non_toxic: 0.892 (37)  \tacc on toxic: 0.679 (190)\tf1_score: 0.799 (227)\n",
      "other_religions acc on non_toxic: 0.853 (34)  \tacc on toxic: 0.632 (68)\tf1_score: 0.741 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.691 (175)\tf1_score: 0.812 (180)\n",
      "white \t\tacc on non_toxic: 0.762 (21)  \tacc on toxic: 0.610 (267)\tf1_score: 0.749 (288)\n",
      "Average accuracy on non toxic for seed is: 0.910\n",
      "Average accuracy on toxic for seed is: 0.642\n",
      "Average accuracy for seed is: 0.787\n",
      "Worst accuracy is 0.600 for demographic group black\n",
      "Average f1 for seed is: 0.741\n",
      "Worst f1 is 0.736 for demographic group female\n",
      "\n",
      "---------------- Seed 10---------------\n",
      "\n",
      "male \t\tacc on non_toxic: 0.930 (86)  \tacc on toxic: 0.626 (254)\tf1_score: 0.759 (340)\n",
      "female \t\tacc on non_toxic: 0.918 (122)  \tacc on toxic: 0.618 (280)\tf1_score: 0.747 (402)\n",
      "LGBTQ \t\tacc on non_toxic: 1.000 (7)  \tacc on toxic: 0.716 (162)\tf1_score: 0.835 (169)\n",
      "christian \tacc on non_toxic: 0.949 (78) \tacc on toxic: 0.635 (159)\tf1_score: 0.765 (237)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muslim \t\tacc on non_toxic: 0.865 (37)  \tacc on toxic: 0.758 (190)\tf1_score: 0.850 (227)\n",
      "other_religions acc on non_toxic: 0.794 (34)  \tacc on toxic: 0.676 (68)\tf1_score: 0.760 (102)\n",
      "black \t\tacc on non_toxic: 0.600 (5)  \tacc on toxic: 0.657 (175)\tf1_score: 0.788 (180)\n",
      "white \t\tacc on non_toxic: 0.762 (21)  \tacc on toxic: 0.610 (267)\tf1_score: 0.749 (288)\n",
      "Average accuracy on non toxic for seed is: 0.900\n",
      "Average accuracy on toxic for seed is: 0.654\n",
      "Average accuracy for seed is: 0.791\n",
      "Worst accuracy is 0.600 for demographic group black\n",
      "Average f1 for seed is: 0.748\n",
      "Worst f1 is 0.747 for demographic group female\n",
      "\n",
      "---------------- Overall evaluation---------------\n",
      "\n",
      "Overall non toxic acc. is: 0.888\n",
      "Overall toxic acc. is: 0.666\n",
      "Overall avg. acc. is: 0.793\n",
      "Overall worst acc. is: 0.400 for demographic group black in seed 4\n",
      "Overall avg. f1. is: 0.751\n",
      "Overall worst f1. is: 0.712 for demographic group other_religions in seed 8\n"
     ]
    }
   ],
   "source": [
    "printEvalResults('finalOwnWilds/logs/50000noDP/civilcomments_split-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "local-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvalResults(preds, labels, data):\n",
    "    worst_acc = 1\n",
    "    worst_f1 = 1\n",
    "    worst_group_acc = ''\n",
    "    worst_group_f1 = ''\n",
    "    toxic_accuracies = []\n",
    "    nonToxic_accuracies = []\n",
    "    overall_toxic = 0\n",
    "    overall_nonToxic = 0\n",
    "\n",
    "    # Go through all but the first two columns and check accuracy\n",
    "    for curCol in data.iloc[:,2:]:\n",
    "        curColArray = np.array(data[curCol].values.tolist())\n",
    "        cur_f1, total_group = checkF1DemoGroup(curColArray, preds, labels)\n",
    "        acc_toxic, acc_nonToxic, total_toxic, total_nonToxic = checkAccDemoGroup(curColArray, preds, labels)\n",
    "        if worst_acc > acc_toxic:\n",
    "            worst_acc = acc_toxic\n",
    "            worst_group_acc = curCol\n",
    "        elif worst_acc > acc_nonToxic:\n",
    "            worst_acc = acc_nonToxic\n",
    "            worst_group_acc = curCol\n",
    "        if worst_f1 > cur_f1:\n",
    "            worst_f1 = cur_f1\n",
    "            worst_group_f1 = curCol\n",
    "        toxic_accuracies.append(acc_toxic*total_toxic)\n",
    "        nonToxic_accuracies.append(acc_nonToxic*total_nonToxic)\n",
    "        overall_toxic += total_toxic\n",
    "        overall_nonToxic += total_nonToxic\n",
    "    avgAcc = CheckAccuracy(preds, labels)\n",
    "    avgF1 = f1_score(labels, preds, zero_division=1)\n",
    "    avgAccToxic = sum(toxic_accuracies)/overall_toxic\n",
    "    avgAccNonToxic = sum(nonToxic_accuracies)/overall_nonToxic\n",
    "    \n",
    "    return avgAcc, avgF1, avgAccToxic, avgAccNonToxic, worst_group_acc, worst_group_f1, worst_acc, worst_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numeric-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveNoDPEvalResults(pathPrefix):\n",
    "    val_labels = np.array(validation_data['toxicity'].values.tolist())\n",
    "    test_labels = np.array(test_data['toxicity'].values.tolist())\n",
    "    \n",
    "    with open('CSVFiles/50000noDPEval.csv','w',newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Seed','n_epochs','Epsilon','Loss function','Optimizer','Learning rate','Batch_size',\n",
    "                         'F1-score test','F1-score test worst','F1-score val','F1-score val worst',\n",
    "                         'Acc toxic test','Acc non-toxic test','Overall acc test','Worst group acc test',\n",
    "                         'Acc toxic val','Acc non-toxic val','Overall acc val','Worst group acc val'])\n",
    "    \n",
    "        for i in range(1, 11):\n",
    "            test_preds, val_preds = loadTestAndValPreds(i, pathPrefix)\n",
    "            avgSeedAcc_val, avgSeedF1_val, avgSeedAccToxic_val, avgSeedAccNonToxic_val, worst_group_acc_val, worst_group_f1_val, worst_acc_val, worst_f1_val = getEvalResults(val_preds, val_labels, validation_data)\n",
    "            avgSeedAcc_test, avgSeedF1_test, avgSeedAccToxic_test, avgSeedAccNonToxic_test, worst_group_acc_test, worst_group_f1_test, worst_acc_test, worst_f1_test = getEvalResults(test_preds, test_labels, test_data)\n",
    "            \n",
    "            writer.writerow([i, 20, 0, 'Cross-entropy', 'AdamW', 1.00E-05,16,\n",
    "                             avgSeedF1_test,f'{worst_f1_test:.3f} ({worst_group_f1_test})',avgSeedF1_val,f'{worst_f1_val:.3f} ({worst_group_f1_val})',\n",
    "                             avgSeedAccToxic_test,avgSeedAccNonToxic_test,avgSeedAcc_test,f'{worst_acc_test:.3f} ({worst_group_acc_test})',\n",
    "                             avgSeedAccToxic_val,avgSeedAccNonToxic_val,avgSeedAcc_val,f'{worst_acc_val:.3f} ({worst_group_acc_val})'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alone-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDPEvalResults(pathPrefix):\n",
    "    val_labels = np.array(validation_data['toxicity'].values.tolist())\n",
    "    test_labels = np.array(test_data['toxicity'].values.tolist())\n",
    "    \n",
    "    with open('CSVFiles/50000DPEvalEvery10.csv','w',newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Seed','n_epochs','Loss function','Optimizer','Learning rate','Batch Size',\n",
    "                         'Virtual batch size','Target eps','Target delta','Sample rate','Dp max grad norm',\n",
    "                         'F1-score test','F1-score test worst','F1-score val','F1-score val worst',\n",
    "                         'Acc toxic test','Acc non-toxic test','Overall acc test','Worst group acc test',\n",
    "                         'Acc toxic val','Acc non-toxic val','Overall acc val','Worst group acc val'])\n",
    "        seeds = [1,5,10,20,30,40,50,60,70,80,90]\n",
    "        #seeds = [100,200,300,400,500,600,700,800,900,1000]\n",
    "        \n",
    "        for i in seeds:\n",
    "            test_preds, val_preds = loadTestAndValPreds(i, pathPrefix)\n",
    "            avgSeedAcc_val, avgSeedF1_val, avgSeedAccToxic_val, avgSeedAccNonToxic_val, worst_group_acc_val, worst_group_f1_val, worst_acc_val, worst_f1_val = getEvalResults(val_preds, val_labels, validation_data)\n",
    "            avgSeedAcc_test, avgSeedF1_test, avgSeedAccToxic_test, avgSeedAccNonToxic_test, worst_group_acc_test, worst_group_f1_test, worst_acc_test, worst_f1_test = getEvalResults(test_preds, test_labels, test_data)\n",
    "            \n",
    "            writer.writerow([i, 20, 'Cross-entropy', 'AdamW', 1.00E-05, 8,\n",
    "                             16, i, 1e-05, 0.01, 1.2,\n",
    "                             avgSeedF1_test,f'{worst_f1_test:.3f} ({worst_group_f1_test})',avgSeedF1_val,f'{worst_f1_val:.3f} ({worst_group_f1_val})',\n",
    "                             avgSeedAccToxic_test,avgSeedAccNonToxic_test,avgSeedAcc_test,f'{worst_acc_test:.3f} ({worst_group_acc_test})',\n",
    "                             avgSeedAccToxic_val,avgSeedAccNonToxic_val,avgSeedAcc_val,f'{worst_acc_val:.3f} ({worst_group_acc_val})'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "australian-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveNoDPEvalResults('finalOwnWilds/logs/50000noDP/civilcomments_split-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fifteen-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDPEvalResults('finalOwnWilds/logs/50000Every100/civilcomments_split-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-final",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
