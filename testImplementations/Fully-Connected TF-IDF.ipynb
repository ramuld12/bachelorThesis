{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal dataset\n",
    "df = pd.read_csv('../CSVFiles/smallDomainDataNotEmbedded.csv')\n",
    "\n",
    "def CleanText(text):\n",
    "    text = text.lower() #Turn all text entries into lower-case\n",
    "    text = re.sub(r'''(https?:\\/\\/www\\.|https?:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,3}[-a-zA-Z0-9()@:%_\\+.~#?&\\//=<>]*''', \"<URL>\", text)\n",
    "    #Replace URL with tag\n",
    "    text = re.sub(r'''[0-9]+[/\\-.]+[0-9]+[/\\-.]+[0-9]+''', \"<DATE>\", text) #Replace dates with tag\n",
    "    text = re.sub(r'''[a-z0-9._%+-]+\\@[a-z0-9.-]+[a-z0-9]\\.[a-z]{1,}''', \"<EMAIL>\", text)\n",
    "    text = re.sub(r'''[0-9]+''', \"<NUM>\", text) #Replace numbers with tag\n",
    "    text = re.sub(r'''[.|,|!|?|\\'|\\''|\\\"|\\n|\\t|\\-|\\(|\\)]''', '', text)\n",
    "    text = re.sub(r'''^\\s+|\\s+$''', '', text) #Remove whitespaces at the end and start of string\n",
    "    text = re.sub(r'''[ ][ ]+|_''', \" \", text) #Remove multiple whitespace\n",
    "    if len(text) <= 0:\n",
    "        text = re.sub(r'''''', \"0\", text) #Remove multiple whitespace\n",
    "    return text\n",
    "\n",
    "df['comment_text'] = df['comment_text'].apply(lambda text: CleanText(text))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating values for training_data\n",
    "training_data = df[df['split'] == 'train']\n",
    "\n",
    "# Getting test_data\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "# Getting validation_data\n",
    "validation_data = df[df['split'] == 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data loaders\n",
    "X_train = np.array(training_data['comment_text'].values.tolist())\n",
    "Y_train = np.array(training_data['toxicity'].values.tolist())\n",
    "\n",
    "X_test = np.array(test_data['comment_text'].values.tolist())\n",
    "Y_test = np.array(test_data['toxicity'].values.tolist())\n",
    "\n",
    "X_val = np.array(validation_data['comment_text'].values.tolist())\n",
    "Y_val = np.array(validation_data['toxicity'].values.tolist())\n",
    "\n",
    "\n",
    "# Creating TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "X_val = X_val.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"LogisticRegression score: \",clf.score(X_test, Y_test))\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron()\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"Perceptron score: \",clf.score(X_test, Y_test))\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"KNN score: \", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set parameters for the model\n",
    "#torch.manual_seed(42) # set fixed random seed for reproducibility\n",
    "batch_size = 16\n",
    "epochs = 1000\n",
    "lr = 0.00001\n",
    "\n",
    "cuda = True # Set this if training on GPU\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Using \"+repr(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_trainloader = []\n",
    "for i in range(len(X_train)):\n",
    "    prepare_trainloader.append([X_train[i], Y_train[i]])\n",
    "    \n",
    "prepare_testloader = []\n",
    "for i in range(len(X_test)):\n",
    "    prepare_testloader.append([X_test[i], Y_test[i]])\n",
    "    \n",
    "prepare_validloader = []\n",
    "for i in range(len(X_val)):\n",
    "    prepare_validloader.append([X_val[i], Y_val[i]])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(prepare_trainloader, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(prepare_validloader, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(prepare_testloader, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckAccuracy(predictions, labels):\n",
    "        acc = 0.0\n",
    "        for i in range(len(predictions)):\n",
    "            if (predictions[i] == labels[i]):\n",
    "                acc += 1\n",
    "        return acc/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(9620, 512), nn.ReLU(), nn.Dropout(p=0.8))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(512, 32), nn.ReLU())\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model parameters\n",
    "model = Net().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# Initialising early stopping criterias\n",
    "early_stopping = 50\n",
    "notImproved = 0\n",
    "bestLoss = None\n",
    "bestModel = None\n",
    "\n",
    "trainArr = []\n",
    "valArr = []\n",
    "\n",
    "bestf1 = 0\n",
    "bestEpoch = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "    \n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        \n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    \n",
    "    trainArr.append(train_loss)\n",
    "    \n",
    "    valid_loss = 0\n",
    "    labs = []\n",
    "    preds = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():        \n",
    "        for batch_idx, data in enumerate(valid_loader):\n",
    "            # get the input\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            \n",
    "            labs.extend(labels)\n",
    "            preds.extend(torch.round(outputs))\n",
    "            \n",
    "            valid_loss += loss_function(outputs, labels).item()\n",
    "    \n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    valArr.append(valid_loss)\n",
    "    \n",
    "    #print(CheckAccuracy(labs, preds))\n",
    "    \n",
    "    if bestLoss == None:\n",
    "        bestLoss = valid_loss\n",
    "    \n",
    "    if valid_loss <= bestLoss:\n",
    "        bestModel = model\n",
    "        bestLoss = valid_loss\n",
    "        notImproved = 0\n",
    "        bestEpoch = epoch\n",
    "    \n",
    "    else:\n",
    "        notImproved +=1\n",
    "        \n",
    "    if notImproved >= early_stopping:\n",
    "        break\n",
    "\n",
    "model = bestModel\n",
    "\n",
    "print(bestEpoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(np.arange(0,len(trainArr)), trainArr, color='r', label='Training loss')\n",
    "plt.scatter(np.arange(0,len(valArr)), valArr, color='g', label='Validation loss')\n",
    "plt.title(\"Training loss vs Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_Scores(preds, labels):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for i in range(len(labs)):\n",
    "        if labels[i]==1 and preds[i]==1:\n",
    "            true_positives += 1\n",
    "        if labels[i]==0 and preds[i]==0:\n",
    "            true_negatives += 1\n",
    "        if labels[i]==0 and preds[i]==1:\n",
    "            false_positives += 1\n",
    "        if labels[i]==1 and preds[i]==0:\n",
    "            false_negatives += 1\n",
    "    print(\"true_positives\", true_positives)\n",
    "    print(\"true_negatives\", true_negatives)\n",
    "    print(\"false_positives\", false_positives)\n",
    "    print(\"false_negatives\", false_negatives)\n",
    "    \n",
    "    return true_positives, true_negatives, false_positives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = []\n",
    "preds = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        \n",
    "        labs.extend(labels)\n",
    "        preds.extend(torch.round(outputs))\n",
    "        \n",
    "print(\"Accuracy on test set: \", CheckAccuracy(labs, preds))\n",
    "\n",
    "f1 = f1_score(torch.Tensor(labs).numpy(), torch.Tensor(preds).numpy(), zero_division=1)\n",
    "print(\"F1_score: \", f1)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "    for batch_idx, data in enumerate(valid_loader):\n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        \n",
    "        labs.extend(labels)\n",
    "        preds.extend(torch.round(outputs))\n",
    "        \n",
    "print(\"Accuracy on validation set: \", CheckAccuracy(labs, preds))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():        \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        \n",
    "        labs.extend(labels)\n",
    "        preds.extend(torch.round(outputs))\n",
    "        \n",
    "print(\"Accuracy on train set: \", CheckAccuracy(labs, preds))\n",
    "\n",
    "\n",
    "\n",
    "true_positives, true_negatives, false_positives, false_negatives = F1_Scores(preds, labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
