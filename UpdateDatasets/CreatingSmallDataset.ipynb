{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all_data_with_identitiesEmbedded.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f5abfb34a8f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Normal dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"all_data_with_identitiesEmbedded.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCleanText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'toxicity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'toxicity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all_data_with_identitiesEmbedded.csv'"
     ]
    }
   ],
   "source": [
    "def CleanText(text):\n",
    "    text = re.sub(r'''[\\[|\\]]''', \"\", text).split()\n",
    "    text = np.array(text, dtype=\"float64\")\n",
    "    return text\n",
    "\n",
    "# Normal dataset\n",
    "df1 = pd.read_csv(\"all_data_with_identitiesEmbedded.csv\")\n",
    "df1['comment_text'] = df1['comment_text'].apply(lambda text: CleanText(text))\n",
    "df1['toxicity'] = df1['toxicity'].apply(lambda text: round(text))\n",
    "\n",
    "# Normal dataset\n",
    "#df2 = pd.read_csv(\"all_data_with_identities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../CSVFiles/all_data_with_identities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal dataset\n",
    "df = pd.read_csv(\"../CSVFiles/all_data_with_identities.csv\")\n",
    "df['toxicity'] = df['toxicity'].apply(lambda text: np.round(text >= 0.5))\n",
    "\n",
    "#df['male'] = df['male'].apply(lambda x: np.round(x >= 0.5))\n",
    "#df['female'] = df['female'].apply(lambda x: np.round(x >= 0.5))\n",
    "#df['LGBTQ'] = df['LGBTQ'].apply(lambda x: np.round(x >= 0.5))\n",
    "#df['christian'] = df['christian'].apply(lambda x: np.round(x >= 0.5))\n",
    "#df['muslim'] = df['muslim'].apply(lambda x: np.round(x >= 0.5))\n",
    "#df['black'] = df['black'].apply(lambda x: np.round(x >= 0.5))\n",
    "#df['white'] = df['white'].apply(lambda x: np.round(x >= 0.5))\n",
    "#df['other_religion'] = df['other_religion'].apply(lambda x: np.round(x >= 0.5))\n",
    "\n",
    "#df = df.loc[:, [\"comment_text\", \"split\", \"toxicity\", \"male\", \"female\", \"LGBTQ\", \"christian\", \"muslim\", \"other_religion\", \"black\", \"white\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>...</th>\n",
       "      <th>asian_latino_etc</th>\n",
       "      <th>disability_any</th>\n",
       "      <th>identity_any</th>\n",
       "      <th>num_identities</th>\n",
       "      <th>more_than_one_identity</th>\n",
       "      <th>na_gender</th>\n",
       "      <th>na_orientation</th>\n",
       "      <th>na_religion</th>\n",
       "      <th>na_race</th>\n",
       "      <th>na_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>627762</td>\n",
       "      <td>OH yes - Were those evil Christian Missionarie...</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-11-26 15:56:03.862109+00</td>\n",
       "      <td>13</td>\n",
       "      <td>627198.0</td>\n",
       "      <td>152737</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5892815</td>\n",
       "      <td>Why is this black racist crap still on the G&amp;M...</td>\n",
       "      <td>val</td>\n",
       "      <td>2017-09-03 23:20:08.226613+00</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>373428</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>416437</td>\n",
       "      <td>even up here.......BLACKS!</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-08-04 16:48:07.175252+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143025</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5137126</td>\n",
       "      <td>Blame men.  There's always an excuse to blame ...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-04-15 19:00:45.032674+00</td>\n",
       "      <td>54</td>\n",
       "      <td>5136907.0</td>\n",
       "      <td>327125</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>855753</td>\n",
       "      <td>And the woman exposing herself saying grab thi...</td>\n",
       "      <td>val</td>\n",
       "      <td>2017-01-18 01:50:57.478867+00</td>\n",
       "      <td>13</td>\n",
       "      <td>849081.0</td>\n",
       "      <td>162008</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447995</th>\n",
       "      <td>447995</td>\n",
       "      <td>1018736</td>\n",
       "      <td>Another man shamming article. If white men did...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-02-20 07:20:49.964620+00</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169202</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447996</th>\n",
       "      <td>447996</td>\n",
       "      <td>340016</td>\n",
       "      <td>\"no matter what is put in front of you regardi...</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-06-06 06:43:04.780968+00</td>\n",
       "      <td>21</td>\n",
       "      <td>339965.0</td>\n",
       "      <td>137961</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447997</th>\n",
       "      <td>447997</td>\n",
       "      <td>919629</td>\n",
       "      <td>The Democrat party aided and abetted by it's M...</td>\n",
       "      <td>test</td>\n",
       "      <td>2017-01-30 02:44:29.168863+00</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164845</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447998</th>\n",
       "      <td>447998</td>\n",
       "      <td>5165492</td>\n",
       "      <td>I just don't find her a very good representati...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-04-22 18:42:02.442987+00</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>328877</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447999</th>\n",
       "      <td>447999</td>\n",
       "      <td>4984105</td>\n",
       "      <td>You know the Trump fanatics are trolling the G...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-03-10 00:55:35.369198+00</td>\n",
       "      <td>54</td>\n",
       "      <td>807615.0</td>\n",
       "      <td>156960</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>448000 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       id  \\\n",
       "0                0   627762   \n",
       "1                1  5892815   \n",
       "2                2   416437   \n",
       "3                3  5137126   \n",
       "4                4   855753   \n",
       "...            ...      ...   \n",
       "447995      447995  1018736   \n",
       "447996      447996   340016   \n",
       "447997      447997   919629   \n",
       "447998      447998  5165492   \n",
       "447999      447999  4984105   \n",
       "\n",
       "                                             comment_text  split  \\\n",
       "0       OH yes - Were those evil Christian Missionarie...   test   \n",
       "1       Why is this black racist crap still on the G&M...    val   \n",
       "2                              even up here.......BLACKS!  train   \n",
       "3       Blame men.  There's always an excuse to blame ...  train   \n",
       "4       And the woman exposing herself saying grab thi...    val   \n",
       "...                                                   ...    ...   \n",
       "447995  Another man shamming article. If white men did...  train   \n",
       "447996  \"no matter what is put in front of you regardi...  train   \n",
       "447997  The Democrat party aided and abetted by it's M...   test   \n",
       "447998  I just don't find her a very good representati...  train   \n",
       "447999  You know the Trump fanatics are trolling the G...  train   \n",
       "\n",
       "                         created_date  publication_id  parent_id  article_id  \\\n",
       "0       2016-11-26 15:56:03.862109+00              13   627198.0      152737   \n",
       "1       2017-09-03 23:20:08.226613+00              54        NaN      373428   \n",
       "2       2016-08-04 16:48:07.175252+00              21        NaN      143025   \n",
       "3       2017-04-15 19:00:45.032674+00              54  5136907.0      327125   \n",
       "4       2017-01-18 01:50:57.478867+00              13   849081.0      162008   \n",
       "...                               ...             ...        ...         ...   \n",
       "447995  2017-02-20 07:20:49.964620+00              54        NaN      169202   \n",
       "447996  2016-06-06 06:43:04.780968+00              21   339965.0      137961   \n",
       "447997  2017-01-30 02:44:29.168863+00              54        NaN      164845   \n",
       "447998  2017-04-22 18:42:02.442987+00              54        NaN      328877   \n",
       "447999  2017-03-10 00:55:35.369198+00              54   807615.0      156960   \n",
       "\n",
       "          rating  funny  ...  asian_latino_etc  disability_any  identity_any  \\\n",
       "0       approved      0  ...                 0               0             1   \n",
       "1       rejected      0  ...                 0               0             1   \n",
       "2       rejected      0  ...                 0               0             1   \n",
       "3       rejected      0  ...                 0               0             1   \n",
       "4       rejected      0  ...                 0               0             1   \n",
       "...          ...    ...  ...               ...             ...           ...   \n",
       "447995  approved      0  ...                 0               0             1   \n",
       "447996  approved      0  ...                 0               0             1   \n",
       "447997  rejected      0  ...                 0               0             0   \n",
       "447998  approved      1  ...                 0               0             1   \n",
       "447999  approved      1  ...                 0               0             1   \n",
       "\n",
       "        num_identities  more_than_one_identity  na_gender  na_orientation  \\\n",
       "0                  1.0                   False          1               1   \n",
       "1                  2.0                    True          1               1   \n",
       "2                  1.0                   False          1               1   \n",
       "3                  2.0                    True          0               1   \n",
       "4                  1.0                   False          0               1   \n",
       "...                ...                     ...        ...             ...   \n",
       "447995             2.0                    True          0               1   \n",
       "447996             2.0                    True          0               1   \n",
       "447997             0.0                   False          1               1   \n",
       "447998             1.0                   False          0               1   \n",
       "447999             1.0                   False          1               1   \n",
       "\n",
       "        na_religion  na_race  na_disability  \n",
       "0                 0        1              1  \n",
       "1                 1        0              1  \n",
       "2                 1        0              1  \n",
       "3                 1        1              1  \n",
       "4                 1        1              1  \n",
       "...             ...      ...            ...  \n",
       "447995            1        0              1  \n",
       "447996            1        1              1  \n",
       "447997            1        1              1  \n",
       "447998            1        1              1  \n",
       "447999            0        1              1  \n",
       "\n",
       "[448000 rows x 59 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxLst = []\n",
    "splitLst = []\n",
    "\n",
    "idxLst.extend(df.query('other_religion>=0.5 and toxicity == 1').index[:30])\n",
    "for i in range(10):\n",
    "    splitLst.append('train')\n",
    "    splitLst.append('test')\n",
    "    splitLst.append('val')\n",
    "    \n",
    "\n",
    "idxLst.extend(df.query('other_religion>=0.5 and toxicity == 0').index[:30])\n",
    "for i in range(10):\n",
    "    splitLst.append('train')\n",
    "    splitLst.append('test')\n",
    "    splitLst.append('val')\n",
    "    \n",
    "    \n",
    "toxicity = [1, 0]\n",
    "\n",
    "for t in toxicity:\n",
    "    count = 0\n",
    "    iterater = 0\n",
    "    currentDF = df.query('toxicity=='+str(t))\n",
    "    currentDFIndexes = currentDF.index\n",
    "    while count < 25000:\n",
    "        if currentDFIndexes[iterater] not in idxLst:\n",
    "            idxLst.append(currentDFIndexes[iterater])\n",
    "            count+=1\n",
    "            if count <= 22000:\n",
    "                splitLst.append('train')\n",
    "            elif count > 22000 and count <= 23500:\n",
    "                splitLst.append('test')\n",
    "            else:\n",
    "                splitLst.append('val')\n",
    "        iterater +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "4002\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4090 is out of bounds for axis 0 with size 4090",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-ab789141b484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mcurrentDFIndexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrentDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m4000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mcurrentDFIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miterater\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midxLst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0midxLst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentDFIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miterater\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4099\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4100\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4101\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4090 is out of bounds for axis 0 with size 4090"
     ]
    }
   ],
   "source": [
    "idxLst = []\n",
    "splitLst = []\n",
    "\n",
    "idxLst.extend(df.query('other_religion>=0.5 and toxicity == 1').index[:30])\n",
    "for i in range(10):\n",
    "    splitLst.append('train')\n",
    "    splitLst.append('test')\n",
    "    splitLst.append('val')\n",
    "    \n",
    "\n",
    "idxLst.extend(df.query('other_religion>=0.5 and toxicity == 0').index[:30])\n",
    "for i in range(10):\n",
    "    splitLst.append('train')\n",
    "    splitLst.append('test')\n",
    "    splitLst.append('val')\n",
    "    \n",
    "    \n",
    "print(len(idxLst))\n",
    "print(len(splitLst))\n",
    "\n",
    "\n",
    "#categories = ['male', 'christian', 'LGBTQ', 'muslim', 'female', 'black', 'white']\n",
    "toxicity = [1, 0]\n",
    "\n",
    "for t in toxicity:\n",
    "    for cat in categories:\n",
    "        \n",
    "        count = 0\n",
    "        iterater = 0\n",
    "        \n",
    "        currentDF = df.query(cat+'>=0.5 and toxicity=='+str(t))\n",
    "        currentDFIndexes = currentDF.index\n",
    "        while count < 4000:\n",
    "            if currentDFIndexes[iterater] not in idxLst:\n",
    "                idxLst.append(currentDFIndexes[iterater])\n",
    "                count+=1\n",
    "                if count <= 3000:\n",
    "                    splitLst.append('train')\n",
    "                elif count > 3000 and count <= 3500:\n",
    "                    splitLst.append('test')\n",
    "                else:\n",
    "                    splitLst.append('val')\n",
    "            iterater +=1\n",
    "        print(iterater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50060\n"
     ]
    }
   ],
   "source": [
    "print(len(idxLst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = df.loc[idxLst]\n",
    "\n",
    "for i, idx in enumerate(idxLst):\n",
    "    newDF.at[idx, 'split'] = splitLst[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.to_csv(r'C:\\Users\\frede\\Desktop\\small50000AllDataWithIdentities.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic samples:  25030.0\n",
      "None-toxic samples:  25030.0\n",
      "\n",
      "\n",
      "Validation samples:  3020\n",
      "Test samples 3020\n",
      "\n",
      "\n",
      "male: 5159.836288897452\n",
      "female: 6026.481274256372\n",
      "LGBTQ: 1945\n",
      "christian: 2740.217357092696\n",
      "muslim: 2646.730185007008\n",
      "other_religion: 216.70208189093648\n",
      "black: 2660.136958801574\n",
      "white: 4044.2291943775836\n"
     ]
    }
   ],
   "source": [
    "tempDF = newDF.loc[:, [\"comment_text\", \"split\", \"toxicity\", \"male\", \"female\", \"LGBTQ\", \"christian\", \"muslim\", \"other_religion\", \"black\", \"white\"]]\n",
    "\n",
    "print('Toxic samples: ', sum(newDF['toxicity']))\n",
    "print('None-toxic samples: ', len(newDF['toxicity'])-sum(newDF['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Validation samples: \", np.sum(newDF['split'] == 'val'))\n",
    "print(\"Test samples\", np.sum(newDF['split'] == 'test'))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for col in tempDF.columns[3:]:\n",
    "    print(col + \": \" + str(np.sum(tempDF[col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "newDF = newDF.loc[:, [\"comment_text\", \"split\", \"toxicity\", \"male\", \"female\", \"LGBTQ\", \"christian\", \"muslim\", \"other_religion\", \"black\", \"white\"]]\n",
    "\n",
    "newDF['male'] = newDF['male'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['female'] = newDF['female'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['LGBTQ'] = newDF['LGBTQ'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['christian'] = newDF['christian'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['muslim'] = newDF['muslim'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['black'] = newDF['black'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['white'] = newDF['white'].apply(lambda x: np.round(x >= 0.5))\n",
    "newDF['other_religion'] = newDF['other_religion'].apply(lambda x: np.round(x >= 0.5))\n",
    "\n",
    "\n",
    "dfToEmbedding= newDF\n",
    "\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)                     \n",
    "bertweet.eval()\n",
    "bertweet.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, idx in enumerate(dfToEmbedding.index):\n",
    "        bertweet.resize_token_embeddings(len(tokenizer))\n",
    "        dfToEmbedding.at[idx, 'comment_text'] = str(bertweet(torch.tensor(tokenizer.encode\n",
    "                                                                  (dfToEmbedding['comment_text'][idx], add_special_tokens=True,\n",
    "                                                                   truncation=True)).to('cuda').unsqueeze(0))[1].detach().cpu().numpy()[0])\n",
    "\n",
    "dfToEmbedding.to_csv(r'C:\\Users\\frede\\Desktop\\50000DomainDataBertweetEmbedded.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Df\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "dfToEmbedding= pd.read_csv(\"../CSVFiles/smallDomainDataNotEmbedded.csv\")\n",
    "\n",
    "\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)                     \n",
    "bertweet.eval()\n",
    "bertweet.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, idx in enumerate(dfToEmbedding.index):\n",
    "        embeds = bertweet(torch.tensor(tokenizer.encode(dfToEmbedding['comment_text'][idx], add_special_tokens=True,\n",
    "                                                                   truncation=True)).to('cuda').unsqueeze(0))[0].squeeze(0)\n",
    "        \n",
    "        if embeds.shape[0] < 200:\n",
    "            embeds = torch.cat((embeds, torch.zeros((200-embeds.shape[0], 768)).to('cuda')))\n",
    "            \n",
    "        elif embeds.shape[0] > 200:\n",
    "            embeds = embeds[0:200]\n",
    "        \n",
    "        dfToEmbedding.at[idx, 'comment_text'] = embeds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "#dfToEmbedding.to_csv(r'C:\\Users\\frede\\Desktop\\smallDomainDataBertweetWordEmbeds.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[-0.018730633, -0.044255268, 0.08673429, -0.0...\n",
       "1       [[-0.012146702, 0.23959883, 0.14874044, -0.115...\n",
       "2       [[-0.04316508, 0.2109584, 0.083563395, -0.0394...\n",
       "3       [[-0.102337755, 0.21968453, 0.2083544, -0.1189...\n",
       "4       [[-0.19849724, 0.0055802, 0.12685396, -0.27278...\n",
       "                              ...                        \n",
       "2855    [[-0.15536311, 0.11322548, 0.14425291, -0.1191...\n",
       "2856    [[-0.112591244, 0.03947962, 0.2966754, -0.3437...\n",
       "2857    [[-0.10124668, 0.15868574, 0.22361813, -0.1241...\n",
       "2858    [[0.034709007, 0.13317995, 0.2726409, -0.31985...\n",
       "2859    [[0.04770079, 0.1619968, 0.19023469, -0.242425...\n",
       "Name: comment_text, Length: 2860, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfToEmbedding['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(dfToEmbedding['comment_text'].values.tolist())\n",
    "Y_train = np.array(dfToEmbedding['toxicity'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
