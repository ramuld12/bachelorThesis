{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('small50000AllDataWithIdentities.csv')\n",
    "df = df.loc[:, [\"comment_text\", \"split\", \"toxicity\", \"male\", \"female\", \"LGBTQ\", \"christian\", \"muslim\", \"other_religions\", \"black\", \"white\"]]\n",
    "df = df[df['split'] == 'test']\n",
    "\n",
    "predictions = pd.read_csv('civilcomments_split-test_seed-1_epoch-best_pred.csv', header=None)\n",
    "predictions = pd.DataFrame({'predictions': np.array(predictions.values.tolist()).flatten()}, index = df.index)\n",
    "\n",
    "df = pd.concat([predictions, df], axis=1)\n",
    "\n",
    "df['male'] = df['male'].apply(lambda x: np.round(x>=0.5))\n",
    "df['female'] = df['female'].apply(lambda x: np.round(x>=0.5))\n",
    "df['LGBTQ'] = df['LGBTQ'].apply(lambda x: np.round(x>=0.5))\n",
    "df['christian'] = df['christian'].apply(lambda x: np.round(x>=0.5))\n",
    "df['muslim'] = df['muslim'].apply(lambda x: np.round(x>=0.5))\n",
    "df['other_religions'] = df['other_religions'].apply(lambda x: np.round(x>=0.5))\n",
    "df['black'] = df['black'].apply(lambda x: np.round(x>=0.5))\n",
    "df['white'] = df['white'].apply(lambda x: np.round(x>=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Toxic samples test data:  1510.0\n",
      "None-toxic samples test data:  1510.0\n",
      "\n",
      "\n",
      "male: 340.0\n",
      "female: 402.0\n",
      "LGBTQ: 169.0\n",
      "christian: 237.0\n",
      "muslim: 227.0\n",
      "other_religions: 102.0\n",
      "black: 180.0\n",
      "white: 288.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print('Toxic samples test data: ', sum(df['toxicity']))\n",
    "print('None-toxic samples test data: ', len(df['toxicity'])-sum(df['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for col in df.columns[3:]:\n",
    "    print(col + \": \" + str(np.sum(df[col])))\n",
    "\n",
    "#df[(df['male'] == 0) & (df['female'] == 0) & (df['LGBTQ'] == 0) & (df['christian'] == 0) & (df['muslim'] == 0) & (df['other_religions'] == 0) & (df['black'] == 0) & (df['white'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckAccuracy(predictions, labels):\n",
    "        acc = 0.0\n",
    "        for i in range(len(predictions)):\n",
    "            if (predictions[i] == labels[i]):\n",
    "                acc += 1\n",
    "        return acc/len(predictions)\n",
    "\n",
    "def F1AndAcc(df):\n",
    "    f1_scores = []\n",
    "    accuracies = []\n",
    "    demographics = []\n",
    "    \n",
    "    for col in df.columns[4:]:\n",
    "        tempdf = df[(df[col] == 1)]        \n",
    "        \n",
    "        labels = np.array(tempdf['toxicity'].values.tolist())\n",
    "        predictions = np.array(tempdf['predictions'].values.tolist())\n",
    "        \n",
    "        f1_scores.append(f1_score(labels, predictions, zero_division=1))\n",
    "        accuracies.append(CheckAccuracy(labels, predictions))   \n",
    "    return np.array(f1_scores), np.array(accuracies)\n",
    "\n",
    "\n",
    "def pRule(df):\n",
    "    pRules = []\n",
    "    \n",
    "    for col in df.columns[3:]:\n",
    "        \n",
    "        tempdfz1 = df[(df[col] == 1)]      \n",
    "        tempdfz0 = df[(df[col] == 0)]\n",
    "        \n",
    "        labelsz1 = np.array(tempdfz1['toxicity'].values.tolist())\n",
    "        predictionsz1 = np.array(tempdfz1['predictions'].values.tolist())\n",
    "        \n",
    "        labelsz0 = np.array(tempdfz0['toxicity'].values.tolist())\n",
    "        predictionsz0 = np.array(tempdfz0['predictions'].values.tolist())\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            \n",
    "            z1Ut1 = np.sum(predictionsz1)/len(df)\n",
    "            pz1 = len(predictionsz1)/len(df)\n",
    "            \n",
    "            z0Ut1 = np.sum(predictionsz0)/len(df)\n",
    "            pz0 = len(predictionsz0)/len(df)\n",
    "            \n",
    "            pscore0 = (z1Ut1/pz1) / (z0Ut1/pz0)\n",
    "            pscore1 = (z0Ut1/pz0) / (z1Ut1/pz1)\n",
    "        \n",
    "        if np.isnan(pscore0) or np.isnan(pscore1):\n",
    "            finalpscore = 0\n",
    "        else:\n",
    "            finalpscore = min(pscore0, pscore1)\n",
    "        \n",
    "        pRules.append(finalpscore)\n",
    "    return pRules\n",
    "\n",
    "\n",
    "def pRuleOwn(df):\n",
    "    pRules = []\n",
    "    \n",
    "    for col in df.columns[4:]:\n",
    "        tempdfz1 = df[(df[col] == 1)]      \n",
    "        tempdfz0 = df[(df[col] == 0)]\n",
    "        \n",
    "        labelsz1 = np.array(tempdfz1['toxicity'].values.tolist())\n",
    "        predictionsz1 = np.array(tempdfz1['predictions'].values.tolist())\n",
    "        \n",
    "        labelsz0 = np.array(tempdfz0['toxicity'].values.tolist())\n",
    "        predictionsz0 = np.array(tempdfz0['predictions'].values.tolist())\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            pscore0 = (np.sum(predictionsz1)/np.sum(labelsz1))/(np.sum(predictionsz0)/np.sum(labelsz0))\n",
    "            pscore1 = (np.sum(predictionsz0)/np.sum(labelsz0))/(np.sum(predictionsz1)/np.sum(labelsz1))\n",
    "        \n",
    "        if np.isnan(pscore0) or np.isnan(pscore1):\n",
    "            finalpscore = 0\n",
    "        else:\n",
    "            finalpscore = min(pscore0, pscore1)\n",
    "        \n",
    "        pRules.append(finalpscore)\n",
    "    return pRules\n",
    "\n",
    "def MinMaxFairness(scores):\n",
    "    return np.max(scores)-np.min(scores)\n",
    "\n",
    "def VarianceFairness(scores):\n",
    "    return np.var(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
