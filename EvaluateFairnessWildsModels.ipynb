{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CSVFiles/all_data_with_identities_50000.csv')\n",
    "df = df.loc[:, [\"comment_text\", \"split\", \"toxicity\", \"male\", \"female\", \"LGBTQ\", \"christian\", \"muslim\", \"other_religions\", \"black\", \"white\"]]\n",
    "df = df[df['split'] == 'test']\n",
    "\n",
    "df['male'] = df['male'].apply(lambda x: np.round(x>=0.5))\n",
    "df['female'] = df['female'].apply(lambda x: np.round(x>=0.5))\n",
    "df['LGBTQ'] = df['LGBTQ'].apply(lambda x: np.round(x>=0.5))\n",
    "df['christian'] = df['christian'].apply(lambda x: np.round(x>=0.5))\n",
    "df['muslim'] = df['muslim'].apply(lambda x: np.round(x>=0.5))\n",
    "df['other_religions'] = df['other_religions'].apply(lambda x: np.round(x>=0.5))\n",
    "df['black'] = df['black'].apply(lambda x: np.round(x>=0.5))\n",
    "df['white'] = df['white'].apply(lambda x: np.round(x>=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestAndValPreds(seed, pathPrefix):\n",
    "    test_preds = []\n",
    "    val_preds = []\n",
    "    test_path = pathPrefix+'test_seed-'+ str(seed) + '_epoch-best_pred.csv'\n",
    "    val_path = pathPrefix+'val_seed-'+ str(seed) + '_epoch-best_pred.csv'\n",
    "    \n",
    "    test_preds = pd.read_csv(test_path, header=None)\n",
    "    val_preds = pd.read_csv(val_path, header=None)\n",
    "\n",
    "    test_preds = pd.DataFrame({'test predictions': np.array(test_preds.values.tolist()).flatten()}, index = df.index)\n",
    "    val_preds = pd.DataFrame({'val predictions': np.array(val_preds.values.tolist()).flatten()}, index = df.index)\n",
    "    \n",
    "    return test_preds, val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Toxic samples test data:  1510.0\n",
      "None-toxic samples test data:  1510.0\n",
      "\n",
      "\n",
      "female: 402.0\n",
      "LGBTQ: 169.0\n",
      "christian: 237.0\n",
      "muslim: 227.0\n",
      "other_religions: 102.0\n",
      "black: 180.0\n",
      "white: 288.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "print('Toxic samples test data: ', sum(df['toxicity']))\n",
    "print('None-toxic samples test data: ', len(df['toxicity'])-sum(df['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for col in df.columns[4:]:\n",
    "    print(col + \": \" + str(np.sum(df[col])))\n",
    "\n",
    "#df[(df['male'] == 0) & (df['female'] == 0) & (df['LGBTQ'] == 0) & (df['christian'] == 0) & (df['muslim'] == 0) & (df['other_religions'] == 0) & (df['black'] == 0) & (df['white'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>LGBTQ</th>\n",
       "      <th>christian</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_religions</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bottom line is all religious extremist of all ...</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So long Europe. So many people are afraid of b...</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Still ignorant Qbcoach15 must be all those Tra...</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The technical term for an atheist who\"stands w...</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Islam means \"submission\". It is expected that ...</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48555</th>\n",
       "      <td>I'll betcha Mark Redwine has had major anger i...</td>\n",
       "      <td>test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48556</th>\n",
       "      <td>Funny you mention the younger woman wearing th...</td>\n",
       "      <td>test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48557</th>\n",
       "      <td>The deficit is out of control.  Listen to the ...</td>\n",
       "      <td>test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48558</th>\n",
       "      <td>I would really, really like to see him and his...</td>\n",
       "      <td>test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48559</th>\n",
       "      <td>This is DES- a City agency that has to pay for...</td>\n",
       "      <td>test</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3020 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comment_text split  toxicity  \\\n",
       "1      Bottom line is all religious extremist of all ...  test       1.0   \n",
       "4      So long Europe. So many people are afraid of b...  test       1.0   \n",
       "7      Still ignorant Qbcoach15 must be all those Tra...  test       1.0   \n",
       "10     The technical term for an atheist who\"stands w...  test       1.0   \n",
       "13     Islam means \"submission\". It is expected that ...  test       1.0   \n",
       "...                                                  ...   ...       ...   \n",
       "48555  I'll betcha Mark Redwine has had major anger i...  test       0.0   \n",
       "48556  Funny you mention the younger woman wearing th...  test       0.0   \n",
       "48557  The deficit is out of control.  Listen to the ...  test       0.0   \n",
       "48558  I would really, really like to see him and his...  test       0.0   \n",
       "48559  This is DES- a City agency that has to pay for...  test       0.0   \n",
       "\n",
       "       male  female  LGBTQ  christian  muslim  other_religions  black  white  \n",
       "1       0.0     0.0    0.0        1.0     1.0              1.0    0.0    0.0  \n",
       "4       0.0     1.0    1.0        0.0     1.0              1.0    0.0    0.0  \n",
       "7       0.0     0.0    0.0        1.0     1.0              1.0    0.0    0.0  \n",
       "10      0.0     0.0    0.0        0.0     0.0              1.0    0.0    0.0  \n",
       "13      0.0     0.0    0.0        0.0     1.0              1.0    0.0    0.0  \n",
       "...     ...     ...    ...        ...     ...              ...    ...    ...  \n",
       "48555   0.0     0.0    0.0        0.0     0.0              0.0    0.0    0.0  \n",
       "48556   0.0     1.0    0.0        0.0     0.0              0.0    0.0    0.0  \n",
       "48557   0.0     0.0    0.0        0.0     0.0              0.0    0.0    0.0  \n",
       "48558   0.0     0.0    0.0        0.0     0.0              0.0    0.0    0.0  \n",
       "48559   0.0     0.0    0.0        0.0     0.0              0.0    0.0    0.0  \n",
       "\n",
       "[3020 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckAccuracy(predictions, labels):\n",
    "        acc = 0.0\n",
    "        for i in range(len(predictions)):\n",
    "            if (predictions[i] == labels[i]):\n",
    "                acc += 1\n",
    "        return acc/len(predictions)\n",
    "\n",
    "def F1AndAcc(df):\n",
    "    f1_scores = []\n",
    "    accuracies = []\n",
    "    demographics = []\n",
    "    \n",
    "    for col in df.columns[4:]:\n",
    "        tempdf = df[(df[col] == 1)]        \n",
    "        \n",
    "        labels = np.array(tempdf['toxicity'].values.tolist())\n",
    "        predictions = np.array(tempdf['test predictions'].values.tolist())\n",
    "        \n",
    "        f1_scores.append(f1_score(labels, predictions, zero_division=1))\n",
    "        accuracies.append(CheckAccuracy(labels, predictions))   \n",
    "    return np.array(f1_scores), np.array(accuracies)\n",
    "\n",
    "\n",
    "def pRule(df):\n",
    "    pRules = []\n",
    "    \n",
    "    for col in df.columns[4:]:\n",
    "        \n",
    "        tempdfz1 = df[(df[col] == 1)]      \n",
    "        tempdfz0 = df[(df[col] == 0)]\n",
    "        \n",
    "        labelsz1 = np.array(tempdfz1['toxicity'].values.tolist())\n",
    "        predictionsz1 = np.array(tempdfz1['test predictions'].values.tolist())\n",
    "        \n",
    "        labelsz0 = np.array(tempdfz0['toxicity'].values.tolist())\n",
    "        predictionsz0 = np.array(tempdfz0['test predictions'].values.tolist())\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            \n",
    "            z1Ut1 = np.sum(predictionsz1)/len(df)\n",
    "            pz1 = len(predictionsz1)/len(df)\n",
    "            \n",
    "            z0Ut1 = np.sum(predictionsz0)/len(df)\n",
    "            pz0 = len(predictionsz0)/len(df)\n",
    "            \n",
    "            pscore0 = (z1Ut1/pz1) / (z0Ut1/pz0)\n",
    "            pscore1 = (z0Ut1/pz0) / (z1Ut1/pz1)\n",
    "        \n",
    "        if np.isnan(pscore0) or np.isnan(pscore1):\n",
    "            finalpscore = 0\n",
    "        else:\n",
    "            finalpscore = min(pscore0, pscore1)\n",
    "        \n",
    "        pRules.append(finalpscore)\n",
    "    return pRules\n",
    "\n",
    "\n",
    "def pRuleOwn(df):\n",
    "    pRules = []\n",
    "    \n",
    "    for col in df.columns[4:]:\n",
    "        tempdfz1 = df[(df[col] == 1)]      \n",
    "        tempdfz0 = df[(df[col] == 0)]\n",
    "        \n",
    "        labelsz1 = np.array(tempdfz1['toxicity'].values.tolist())\n",
    "        predictionsz1 = np.array(tempdfz1['test predictions'].values.tolist())\n",
    "        \n",
    "        labelsz0 = np.array(tempdfz0['toxicity'].values.tolist())\n",
    "        predictionsz0 = np.array(tempdfz0['test predictions'].values.tolist())\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            pscore0 = (np.sum(predictionsz1)/np.sum(labelsz1))/(np.sum(predictionsz0)/np.sum(labelsz0))\n",
    "            pscore1 = (np.sum(predictionsz0)/np.sum(labelsz0))/(np.sum(predictionsz1)/np.sum(labelsz1))\n",
    "        \n",
    "        if np.isnan(pscore0) or np.isnan(pscore1):\n",
    "            finalpscore = 0\n",
    "        else:\n",
    "            finalpscore = min(pscore0, pscore1)\n",
    "        \n",
    "        pRules.append(finalpscore)\n",
    "    return pRules\n",
    "\n",
    "def MinMaxFairness(scores):\n",
    "    return np.max(scores)-np.min(scores)\n",
    "\n",
    "def VarianceFairness(scores):\n",
    "    return np.var(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8371452229933416\n",
      "0.9249540737034412\n"
     ]
    }
   ],
   "source": [
    "prulearrmean = []\n",
    "prulearrmin = []\n",
    "accArr = []\n",
    "f1Arr = []\n",
    "\n",
    "\n",
    "for i in range(1,11):\n",
    "    test_preds, val_preds = loadTestAndValPreds(i, 'finalOwnWilds/logs/50000noDP/civilcomments_split-')\n",
    "    temp_df = pd.concat([test_preds, df], axis=1)\n",
    "    prulearrmean.append(np.array(pRuleOwn(temp_df)).mean())\n",
    "    prulearrmin.append(np.min(np.array(pRuleOwn(temp_df))))\n",
    "    f1, acc = F1AndAcc(temp_df)\n",
    "    accArr.append(acc)\n",
    "    f1Arr.append(f1)\n",
    "    \n",
    "prulearrmin = np.array(prulearrmin).mean()\n",
    "prulearrmean = np.array(prulearrmean).mean()\n",
    "\n",
    "print(prulearrmin)\n",
    "print(prulearrmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "varF1 = []\n",
    "minMaxF1 = []\n",
    "pRuleOwnMeanArr = []\n",
    "pRuleOwnMinArr = []\n",
    "pRuleMeanArr = []\n",
    "pRuleMinArr = []\n",
    "accuracy = []\n",
    "for i in range(1,11):\n",
    "    test_preds, val_preds = loadTestAndValPreds(i, 'finalOwnWilds/logs/50000noDP/civilcomments_split-')\n",
    "    temp_df = pd.concat([test_preds, df], axis=1)\n",
    "    f1_scores, accuracies = F1AndAcc(temp_df)\n",
    "\n",
    "    own = pRuleOwn(temp_df)       \n",
    "    pRuleOwnMinArr.append(np.min(own))\n",
    "    pRuleOwnMeanArr.append(np.mean(own))\n",
    "\n",
    "    official = pRule(temp_df)\n",
    "    pRuleMinArr.append(np.min(official))\n",
    "    pRuleMeanArr.append(np.mean(official))\n",
    "\n",
    "    #F1 Variance\n",
    "    varF1.append(VarianceFairness(f1_scores))\n",
    "\n",
    "    #F1 Min Max\n",
    "    minMaxF1.append(MinMaxFairness(f1_scores))\n",
    "\n",
    "    labels = np.array(temp_df['toxicity'].values.tolist())\n",
    "    preds = np.array(temp_df['test predictions'].values.tolist())\n",
    "    accuracy.append(CheckAccuracy(preds,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varF1:\t\t 0.0015\n",
      "minMaxF1:\t 0.1116\n",
      "pRuleOwnMeanArr: 0.9250\n",
      "pRuleOwnMinArr:\t 0.8371\n",
      "pRuleMeanArr:\t 0.5647\n",
      "pRuleMinArr:\t 0.4336\n",
      "Avg accuracy:\t 0.7934\n"
     ]
    }
   ],
   "source": [
    "print(f'varF1:\\t\\t {np.mean(varF1):.4f}')\n",
    "print(f'minMaxF1:\\t {np.mean(minMaxF1):.4f}')\n",
    "print(f'pRuleOwnMeanArr: {np.mean(pRuleOwnMeanArr):.4f}')\n",
    "print(f'pRuleOwnMinArr:\\t {np.mean(pRuleOwnMinArr):.4f}')\n",
    "print(f'pRuleMeanArr:\\t {np.mean(pRuleMeanArr):.4f}')\n",
    "print(f'pRuleMinArr:\\t {np.mean(pRuleMinArr):.4f}')\n",
    "print(f'Avg accuracy:\\t {np.mean(accuracy):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
