{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanText(text):\n",
    "    text = re.sub(r'''[\\[|\\]]''', \"\", text).split()\n",
    "    text = np.array(text, dtype=\"float64\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal dataset\n",
    "df = pd.read_csv('CSVFiles/small50000DomainDataBertweetEmbedded.csv')\n",
    "df['comment_text'] = df['comment_text'].apply(lambda text: CleanText(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-citation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating values for training_data\n",
    "training_data = df[df['split'] == 'train']\n",
    "\n",
    "# Getting test_data\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "# Getting validation_data\n",
    "validation_data = df[df['split'] == 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-belief",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Toxic samples training data: ', sum(training_data['toxicity']))\n",
    "print('None-toxic samples training data: ', len(training_data['toxicity'])-sum(training_data['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Toxic samples validation data: ', sum(validation_data['toxicity']))\n",
    "print('None-toxic samples validation data: ', len(validation_data['toxicity'])-sum(validation_data['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Toxic samples test data: ', sum(test_data['toxicity']))\n",
    "print('None-toxic samples test data: ', len(test_data['toxicity'])-sum(test_data['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for col in training_data.columns[3:]:\n",
    "    print(col + \": \" + str(np.sum(training_data[col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckAccuracy(predictions, labels):\n",
    "        acc = 0.0\n",
    "        for i in range(len(predictions)):\n",
    "            if (predictions[i] == labels[i]):\n",
    "                acc += 1\n",
    "        return acc/len(predictions)\n",
    "    \n",
    "def CreatePlot(trainArr, valArr, path):\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(0,len(trainArr)), trainArr, color='r', label='Training loss')\n",
    "    plt.scatter(np.arange(0,len(valArr)), valArr, color='g', label='Validation loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='upper right')\n",
    "    print(path)\n",
    "    plt.savefig(path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set parameters for the model\n",
    "#torch.manual_seed(1) # set fixed random seed for reproducibility\n",
    "cuda = True # Set this if training on GPU\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Using \"+repr(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data loaders\n",
    "X_train = np.array(training_data['comment_text'].values.tolist())\n",
    "Y_train = np.array(training_data['toxicity'].values.tolist())\n",
    "\n",
    "X_test = np.array(test_data['comment_text'].values.tolist())\n",
    "Y_test = np.array(test_data['toxicity'].values.tolist())\n",
    "\n",
    "X_val = np.array(validation_data['comment_text'].values.tolist())\n",
    "Y_val = np.array(validation_data['toxicity'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(768, 64), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(64, 32), nn.ReLU())\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-brown",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Train(model, train_loader, valid_loader, loss_function, optimizer, learning_rate, early_stopping=50, epochs=1000):\n",
    "    \n",
    "    # Setting up model parameters\n",
    "    model = model.to(device)\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if optimizer == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_function = loss_function\n",
    "\n",
    "    # Initialising early stopping criterias\n",
    "    early_stopping = early_stopping\n",
    "    notImproved = 0\n",
    "    \n",
    "    bestLoss = None\n",
    "    bestModel = None\n",
    "\n",
    "    trainArr = []\n",
    "    valArr = []\n",
    "\n",
    "    bestf1 = 0\n",
    "    bestEpoch = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "            # get the input\n",
    "            inputs, labels = data\n",
    "    \n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(1)\n",
    "        \n",
    "            loss = loss_function(outputs, labels)\n",
    "        \n",
    "            loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            train_loss += loss.item()\n",
    "    \n",
    "        train_loss /= len(train_loader.dataset)    \n",
    "        trainArr.append(train_loss)\n",
    "    \n",
    "        valid_loss = 0\n",
    "        labs = []\n",
    "        preds = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():        \n",
    "            for batch_idx, data in enumerate(valid_loader):\n",
    "                # get the input\n",
    "                inputs, labels = data           \n",
    "            \n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).float()\n",
    "                \n",
    "                outputs = model(inputs).squeeze()\n",
    "                \n",
    "                labs.extend(labels)\n",
    "                preds.extend(torch.round(outputs))\n",
    "            \n",
    "                valid_loss += loss_function(outputs, labels).item()\n",
    "        \n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        valArr.append(valid_loss)\n",
    "        \n",
    "        if bestLoss == None:\n",
    "            bestLoss = valid_loss\n",
    "    \n",
    "        if valid_loss <= bestLoss:\n",
    "            bestModel = torch.save(model, 'currentModel.pth')\n",
    "            bestLoss = valid_loss\n",
    "            notImproved = 0\n",
    "            bestEpoch = epoch\n",
    "        else:\n",
    "            notImproved +=1\n",
    "        \n",
    "        if notImproved >= early_stopping:\n",
    "            break\n",
    "\n",
    "    model = torch.load('currentModel.pth')\n",
    "    return model, valArr, trainArr, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-awareness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DemoAndF1AndAcc(class_model, pca_model, df):\n",
    "    f1_scores = []\n",
    "    accuracies = []\n",
    "    demographics = []\n",
    "    \n",
    "    for col in df.columns[3:]:\n",
    "        tempdf = df[(df[col] == 1)]\n",
    "        \n",
    "        tempX = np.array(tempdf['comment_text'].values.tolist())\n",
    "        tempY = np.array(tempdf['toxicity'].values.tolist())\n",
    "        \n",
    "        transformedXTest = pca_model.transform(tempX)\n",
    "        prep_testloader = []\n",
    "        for i in range(len(tempX)):\n",
    "            prep_testloader.append([transformedXTest[i], Y_train[i]])\n",
    "        \n",
    "        test_loader = torch.utils.data.DataLoader(prep_testloader, batch_size=32, shuffle=False)\n",
    "        \n",
    "        labs = []\n",
    "        preds = []\n",
    "        class_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(test_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).float()\n",
    "                outputs = class_model(inputs).squeeze()\n",
    "                labs.extend(labels)\n",
    "                preds.extend(torch.round(outputs))\n",
    "                \n",
    "        labs = torch.Tensor(labs).detach().numpy()\n",
    "        preds = torch.Tensor(preds).detach().numpy()\n",
    "        \n",
    "        f1_scores.append(f1_score(labs, preds, zero_division=1))\n",
    "        accuracies.append(CheckAccuracy(labs, preds))\n",
    "        demographics.append(col)\n",
    "            \n",
    "    return np.array(demographics), np.array(f1_scores), np.array(accuracies)\n",
    "\n",
    "def pRule(class_model, pca_model, df):\n",
    "    pRules = []\n",
    "    \n",
    "    for col in df.columns[3:]:\n",
    "        \n",
    "        tempdfz1 = df[(df[col] == 1)]\n",
    "        tempdfz0 = df[(df[col] == 0)]\n",
    "        \n",
    "        tempXz1 = np.array(tempdfz1['comment_text'].values.tolist())\n",
    "        tempYz1 = np.array(tempdfz1['toxicity'].values.tolist())\n",
    "        \n",
    "        tempXz0 = np.array(tempdfz0['comment_text'].values.tolist())\n",
    "        tempYz0 = np.array(tempdfz0['toxicity'].values.tolist())\n",
    "        \n",
    "        # For z=1\n",
    "        transformedtempXz1 = pca_model.transform(tempXz1)\n",
    "        prep_testloaderz1 = []\n",
    "        for i in range(len(transformedtempXz1)):\n",
    "            prep_testloaderz1.append([transformedtempXz1[i], tempYz1[i]])\n",
    "            transformedtempXz1 = pca_model.transform(tempXz1)\n",
    "        test_loaderz1 = torch.utils.data.DataLoader(prep_testloaderz1, batch_size=32, shuffle=False)\n",
    "        \n",
    "        labsz1 = []\n",
    "        predsz1 = []\n",
    "        class_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(test_loaderz1):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).float()\n",
    "                outputs = class_model(inputs).squeeze()\n",
    "                labsz1.extend(labels)\n",
    "                predsz1.extend(torch.round(outputs))\n",
    "        \n",
    "        # For z=0\n",
    "        transformedtempXz0 = pca_model.transform(tempXz0)\n",
    "        prep_testloaderz0 = []\n",
    "        for i in range(len(transformedtempXz0)):\n",
    "            prep_testloaderz0.append([transformedtempXz0[i], tempYz0[i]])\n",
    "        test_loaderz0 = torch.utils.data.DataLoader(prep_testloaderz0, batch_size=32, shuffle=False)\n",
    "        \n",
    "        labsz0 = []\n",
    "        predsz0 = []\n",
    "        class_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(test_loaderz0):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).float()\n",
    "                outputs = class_model(inputs).squeeze()\n",
    "                labsz0.extend(labels)\n",
    "                predsz0.extend(torch.round(outputs))\n",
    "        \n",
    "        predictionsz1 = torch.Tensor(predsz1).detach().numpy()\n",
    "        predictionsz0 = torch.Tensor(predsz0).detach().numpy()\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            \n",
    "            z1Ut1 = np.sum(predictionsz1)/len(df)\n",
    "            pz1 = len(predictionsz1)/len(df)\n",
    "            \n",
    "            z0Ut1 = np.sum(predictionsz0)/len(df)\n",
    "            pz0 = len(predictionsz0)/len(df)\n",
    "\n",
    "            pscore0 = (z1Ut1/pz1) / (z0Ut1/pz0)\n",
    "            pscore1 = (z0Ut1/pz0) / (z1Ut1/pz1)\n",
    "        \n",
    "        if np.isnan(pscore0) or np.isnan(pscore1):\n",
    "            finalpscore = 0\n",
    "        else:\n",
    "            finalpscore = min(pscore0, pscore1)\n",
    "        \n",
    "        pRules.append(finalpscore)\n",
    "    return pRules\n",
    "\n",
    "def pRuleOwn(class_model, pca_model, df):\n",
    "    pRules = []\n",
    "    \n",
    "    for col in df.columns[3:]:\n",
    "        tempdfz1 = df[(df[col] == 1)]      \n",
    "        tempdfz0 = df[(df[col] == 0)]\n",
    "        \n",
    "        tempXz1 = np.array(tempdfz1['comment_text'].values.tolist())\n",
    "        tempYz1 = np.array(tempdfz1['toxicity'].values.tolist())\n",
    "        \n",
    "        tempXz0 = np.array(tempdfz0['comment_text'].values.tolist())\n",
    "        tempYz0 = np.array(tempdfz0['toxicity'].values.tolist())\n",
    "        \n",
    "        # For z=1\n",
    "        transformedtempXz1 = pca_model.transform(tempXz1)\n",
    "        prep_testloaderz1 = []\n",
    "        for i in range(len(transformedtempXz1)):\n",
    "            prep_testloaderz1.append([transformedtempXz1[i], tempYz1[i]])\n",
    "            transformedtempXz1 = pca_model.transform(tempXz1)\n",
    "        test_loaderz1 = torch.utils.data.DataLoader(prep_testloaderz1, batch_size=32, shuffle=False)\n",
    "        \n",
    "        labsz1 = []\n",
    "        predsz1 = []\n",
    "        class_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(test_loaderz1):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).float()\n",
    "                outputs = class_model(inputs).squeeze()\n",
    "                labsz1.extend(labels)\n",
    "                predsz1.extend(torch.round(outputs))\n",
    "        \n",
    "        # For z=0\n",
    "        transformedtempXz0 = pca_model.transform(tempXz0)\n",
    "        prep_testloaderz0 = []\n",
    "        for i in range(len(transformedtempXz0)):\n",
    "            prep_testloaderz0.append([transformedtempXz0[i], tempYz0[i]])\n",
    "        test_loaderz0 = torch.utils.data.DataLoader(prep_testloaderz0, batch_size=32, shuffle=False)\n",
    "        \n",
    "        labsz0 = []\n",
    "        predsz0 = []\n",
    "        class_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(test_loaderz0):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).float()\n",
    "                outputs = class_model(inputs).squeeze()\n",
    "                labsz0.extend(labels)\n",
    "                predsz0.extend(torch.round(outputs))\n",
    "        \n",
    "        predictionsz1 = torch.Tensor(predsz1).numpy()\n",
    "        predictionsz0 = torch.Tensor(predsz0).numpy()\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            \n",
    "            pscore0 = (np.sum(predictionsz1)/np.sum(tempYz1))/(np.sum(predictionsz0)/np.sum(tempYz0))\n",
    "            pscore1 = (np.sum(predictionsz0)/np.sum(tempYz0))/(np.sum(predictionsz1)/np.sum(tempYz1))\n",
    "        \n",
    "        if np.isnan(pscore0) or np.isnan(pscore1):\n",
    "            finalpscore = 0\n",
    "        else:\n",
    "            finalpscore = min(pscore0, pscore1)\n",
    "        \n",
    "        pRules.append(finalpscore)\n",
    "    return pRules\n",
    "\n",
    "def MinMaxFairness(scores):\n",
    "    return np.max(scores)-np.min(scores)\n",
    "\n",
    "def VarianceFairness(scores):\n",
    "    return np.var(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-supervisor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathArr = []\n",
    "\n",
    "for filePath in glob.glob('PCAmodels1000/*'):\n",
    "    pathArr.append(filePath)\n",
    "    epsilons.append(filePath[14:-4].replace(',', '.'))\n",
    "\n",
    "epsilons = np.array(epsilons, dtype=float)\n",
    "pathArr = np.array(pathArr)\n",
    "\n",
    "# sort by epsilons\n",
    "sortedIndexes = np.argsort(epsilons)\n",
    "epsilons = epsilons[sortedIndexes]\n",
    "pathArr = pathArr[sortedIndexes]\n",
    "\n",
    "print(epsilons)\n",
    "print(pathArr)\n",
    "\n",
    "varF1 = []\n",
    "varAcc = []\n",
    "minMaxF1 = []\n",
    "minMaxAcc = []\n",
    "pRuleOwnMeanArr = []\n",
    "pRuleOwnMinArr = []\n",
    "pRuleMeanArr = []\n",
    "pRuleMinArr = []\n",
    "accuracy = []\n",
    "\n",
    "for j, path in enumerate(pathArr):\n",
    "    print(j)\n",
    "    PCAmodel = torch.load(path)\n",
    "    transformedXTrain = PCAmodel.transform(X_train)\n",
    "    transformedXVal = PCAmodel.transform(X_val)\n",
    "    \n",
    "    prepare_trainloader = []\n",
    "    for i in range(len(X_train)):\n",
    "        prepare_trainloader.append([transformedXTrain[i], Y_train[i]])\n",
    "    \n",
    "    prepare_validloader = []\n",
    "    for i in range(len(X_val)):\n",
    "        prepare_validloader.append([transformedXVal[i], Y_val[i]])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(prepare_trainloader, batch_size=32, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(prepare_validloader, batch_size=32, shuffle=False)\n",
    "        \n",
    "    model, valLossArr , trainLossArr, epoch = Train(Net(), train_loader, valid_loader, nn.MSELoss(), 'AdamW', 0.000001, early_stopping=50, epochs=20000)\n",
    "    \n",
    "    torch.save(model, 'PrivateReduction50PCA/private' + path[14:])\n",
    "    CreatePlot(trainLossArr, valLossArr, 'PrivateReduction50PCA/private'+ path[14:-4] + '.png')\n",
    "    \n",
    "    demographics, f1_scores, accuracies = DemoAndF1AndAcc(model, PCAmodel, test_data)\n",
    "    \n",
    "    own = pRuleOwn(model, PCAmodel, test_data) \n",
    "    pRuleOwnMinArr.append(np.min(own))\n",
    "    pRuleOwnMeanArr.append(np.mean(own))\n",
    "    \n",
    "    official = pRule(model, PCAmodel, test_data)\n",
    "    pRuleMinArr.append(np.min(official))\n",
    "    pRuleMeanArr.append(np.mean(official))\n",
    "    \n",
    "    #F1 Min Max\n",
    "    minMaxF1.append(MinMaxFairness(f1_scores))\n",
    "    \n",
    "    #F1 Variance\n",
    "    varF1.append(VarianceFairness(f1_scores))\n",
    "    \n",
    "    transformedXtest = PCAmodel.transform(X_test)\n",
    "    prepare_testloader = []\n",
    "    for i in range(len(X_test)):\n",
    "        prepare_testloader.append([transformedXtest[i], Y_test[i]])\n",
    "    test_loader = torch.utils.data.DataLoader(prepare_testloader, batch_size=32, shuffle=False)\n",
    "    \n",
    "    labs = []\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            labs.extend(labels)\n",
    "            preds.extend(torch.round(outputs))\n",
    "    testAcc = CheckAccuracy(labs, preds)\n",
    "    print(\"Accuracy on test set: \", testAcc)\n",
    "    accuracy.append(testAcc)\n",
    "    \n",
    "varF1 = np.array(varF1)\n",
    "minMaxF1 = np.array(minMaxF1)\n",
    "pRuleOwnMeanArr = np.array(pRuleOwnMeanArr)\n",
    "pRuleOwnMinArr = np.array(pRuleOwnMinArr)\n",
    "pRuleMeanArr = np.array(pRuleMeanArr)\n",
    "pRuleMinArr = np.array(pRuleMinArr)\n",
    "accuracy = np.array(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = []\n",
    "pathArrPCA = []\n",
    "\n",
    "for filePath in glob.glob('PCAmodels1000/*'):\n",
    "    pathArrPCA.append(filePath)\n",
    "    epsilons.append(filePath[14:-4].replace(',', '.'))\n",
    "\n",
    "epsilons = np.array(epsilons, dtype=float)\n",
    "pathArrPCA = np.array(pathArrPCA)\n",
    "\n",
    "# sort by epsilons\n",
    "sortedIndexes = np.argsort(epsilons)\n",
    "epsilons = epsilons[sortedIndexes]\n",
    "pathArrPCA = pathArrPCA[sortedIndexes]\n",
    "\n",
    "\n",
    "pathArrFC = []\n",
    "epsilons = []\n",
    "for filePath in glob.glob('PrivateModels50000/Models/*'):\n",
    "    epsilons.append(filePath[33:-4].replace(',', '.'))\n",
    "    pathArrFC.append(filePath)\n",
    "    \n",
    "epsilons = np.array(epsilons, dtype=float)\n",
    "pathArrFC = np.array(pathArrFC)\n",
    "\n",
    "# sort by epsilons\n",
    "sortedIndexes = np.argsort(epsilons)\n",
    "epsilons = epsilons[sortedIndexes]\n",
    "pathArrFC = pathArrFC[sortedIndexes]\n",
    "\n",
    "varF1 = []\n",
    "varAcc = []\n",
    "minMaxF1 = []\n",
    "minMaxAcc = []\n",
    "pRuleOwnMeanArr = []\n",
    "pRuleOwnMinArr = []\n",
    "pRuleMeanArr = []\n",
    "pRuleMinArr = []\n",
    "accuracy = []\n",
    "\n",
    "for i in range(len(pathArrFC)):\n",
    "    print(epsilons[i])\n",
    "    \n",
    "    PCAmodel = torch.load(pathArrPCA[i])\n",
    "    model = torch.load(pathArrFC[i])\n",
    "    \n",
    "    demographics, f1_scores, accuracies = DemoAndF1AndAcc(model, PCAmodel, test_data)\n",
    "    \n",
    "    own = pRuleOwn(model, PCAmodel, test_data) \n",
    "    pRuleOwnMinArr.append(np.min(own))\n",
    "    pRuleOwnMeanArr.append(np.mean(own))\n",
    "    \n",
    "    official = pRule(model, PCAmodel, test_data)\n",
    "    pRuleMinArr.append(np.min(official))\n",
    "    pRuleMeanArr.append(np.mean(official))\n",
    "    \n",
    "    #F1 Min Max\n",
    "    minMaxF1.append(MinMaxFairness(f1_scores))\n",
    "    \n",
    "    #F1 Variance\n",
    "    varF1.append(VarianceFairness(f1_scores))\n",
    "    \n",
    "    transformedXtest = PCAmodel.transform(X_test)\n",
    "    prepare_testloader = []\n",
    "    for i in range(len(X_test)):\n",
    "        prepare_testloader.append([transformedXtest[i], Y_test[i]])\n",
    "    test_loader = torch.utils.data.DataLoader(prepare_testloader, batch_size=32, shuffle=False)\n",
    "    \n",
    "    labs = []\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            labs.extend(labels)\n",
    "            preds.extend(torch.round(outputs))\n",
    "    testAcc = CheckAccuracy(labs, preds)\n",
    "    print(\"Accuracy on test set: \", testAcc)\n",
    "    accuracy.append(testAcc)\n",
    "    \n",
    "varF1 = np.array(varF1)\n",
    "minMaxF1 = np.array(minMaxF1)\n",
    "pRuleOwnMeanArr = np.array(pRuleOwnMeanArr)\n",
    "pRuleOwnMinArr = np.array(pRuleOwnMinArr)\n",
    "pRuleMeanArr = np.array(pRuleMeanArr)\n",
    "pRuleMinArr = np.array(pRuleMinArr)\n",
    "accuracy = np.array(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-allocation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLOT OWN P-RULE MEAN\n",
    "\n",
    "m, b = np.polyfit(epsilons, pRuleOwnMeanArr, 1)\n",
    "\n",
    "X2 = sm.add_constant(epsilons)\n",
    "est = sm.OLS(pRuleOwnMeanArr, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epsilons, pRuleOwnMeanArr, color='r')\n",
    "plt.plot(epsilons, m*epsilons + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * x + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Mean p%-ruleM score\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.savefig('PCAOwnMean.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OWN P-RULE MIN\n",
    "\n",
    "m, b = np.polyfit(epsilons, pRuleOwnMinArr, 1)\n",
    "\n",
    "X2 = sm.add_constant(epsilons)\n",
    "est = sm.OLS(pRuleOwnMinArr, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epsilons, pRuleOwnMinArr, color='r')\n",
    "plt.plot(epsilons, m*epsilons + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * x + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Worst p%-ruleM score\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.savefig('PCAOwnMin.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OFFICIAL P-RULE MIN\n",
    "m, b = np.polyfit(epsilons, pRuleMinArr, 1)\n",
    "\n",
    "X2 = sm.add_constant(epsilons)\n",
    "est = sm.OLS(pRuleMinArr, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epsilons, pRuleMinArr, color='r')\n",
    "plt.plot(epsilons, m*epsilons + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * x + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Worst p%-rule score\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.savefig('PCAOfficialMin.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-origin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLOT OFFICIAL P-RULE Mean\n",
    "m, b = np.polyfit(epsilons, pRuleMeanArr, 1)\n",
    "\n",
    "X2 = sm.add_constant(epsilons)\n",
    "est = sm.OLS(pRuleMeanArr, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "plt.figure()\n",
    "plt.scatter(epsilons, pRuleMeanArr, color='r')\n",
    "plt.plot(epsilons, m*epsilons + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * x + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Mean p%-rule score\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.savefig('PCAfficialMean.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-disabled",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m, b = np.polyfit(epsilons, varF1, 1)\n",
    "\n",
    "X2 = sm.add_constant(epsilons)\n",
    "est = sm.OLS(varF1, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epsilons, varF1, color='r')\n",
    "plt.plot(epsilons, m*epsilons + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * x + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Variance fairness\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.savefig('PCAvarf1.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = np.polyfit(epsilons, minMaxF1, 1)\n",
    "\n",
    "X2 = sm.add_constant(epsilons)\n",
    "est = sm.OLS(minMaxF1, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epsilons, minMaxF1, color='r')\n",
    "plt.plot(epsilons, m*epsilons + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * x + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"maxmin fairness\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.savefig('PCAminmaxf1.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = np.polyfit(epsilons, accuracy, 1)\n",
    "\n",
    "X2 = sm.add_constant(epsilons)\n",
    "est = sm.OLS(accuracy, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "m, b = np.polyfit(epsilons, accuracy, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(epsilons, accuracy, color='r')\n",
    "plt.plot(epsilons, m*epsilons + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * x + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Accuracy\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=10)\n",
    "plt.savefig('PCAaccuracy.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-macedonia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DPPCA.npy', 'wb') as f:\n",
    "    np.save(f, epsilons)\n",
    "    np.save(f, varF1)\n",
    "    np.save(f, minMaxF1)\n",
    "    np.save(f, pRuleOwnMeanArr)\n",
    "    np.save(f, pRuleOwnMinArr)\n",
    "    np.save(f, pRuleMeanArr)\n",
    "    np.save(f, pRuleMinArr)\n",
    "    np.save(f, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DPPCA.npy', 'rb') as f:\n",
    "    epsilons = np.load(f)\n",
    "    varF1 = np.load(f)\n",
    "    minMaxF1 = np.load(f)\n",
    "    pRuleOwnMeanArr = np.load(f)\n",
    "    pRuleOwnMinArr = np.load(f)\n",
    "    pRuleMeanArr = np.load(f)\n",
    "    pRuleMinArr = np.load(f)\n",
    "    accuracy = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-sigma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
