{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from diffprivlib.models import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanText(text):\n",
    "    text = re.sub(r'''[\\[|\\]]''', \"\", text).split()\n",
    "    text = np.array(text, dtype=\"float64\")\n",
    "    return text\n",
    "\n",
    "# Normal dataset\n",
    "df = pd.read_csv('CSVFiles/small10000DomainDataBertweetEmbedded.csv')\n",
    "\n",
    "df['comment_text'] = df['comment_text'].apply(lambda text: CleanText(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating values for training_data, since no need for val set\n",
    "training_data = df[(df['split'] == 'train') | (df['split'] == 'val')]\n",
    "\n",
    "# Getting test_data\n",
    "test_data = df[df['split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Toxic samples training data: ', sum(training_data['toxicity']))\n",
    "print('None-toxic samples training data: ', len(training_data['toxicity'])-sum(training_data['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Toxic samples test data: ', sum(test_data['toxicity']))\n",
    "print('None-toxic samples test data: ', len(test_data['toxicity'])-sum(test_data['toxicity']))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for col in training_data.columns[3:]:\n",
    "    print(col + \": \" + str(np.sum(training_data[col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckAccuracy(predictions, labels):\n",
    "        acc = 0.0\n",
    "        for i in range(len(predictions)):\n",
    "            if (predictions[i] == labels[i]):\n",
    "                acc += 1\n",
    "        return acc/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data loaders\n",
    "X_train = np.array(training_data['comment_text'].values.tolist())\n",
    "Y_train = np.array(training_data['toxicity'].values.tolist())\n",
    "\n",
    "X_test = np.array(test_data['comment_text'].values.tolist())\n",
    "Y_test = np.array(test_data['toxicity'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.linspace(0.1, 40, 400)\n",
    "\n",
    "allModelLsts = []\n",
    "for i in range(15):\n",
    "    modelLst = []\n",
    "    for eps in epsilons:\n",
    "        #clf = LogisticRegression(epsilon=eps, max_iter=30000, data_norm=4.979391198106228)\n",
    "        clf = LogisticRegression(epsilon=eps, max_iter=30000, data_norm=5.069251772214575)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        modelLst.append(clf)\n",
    "    allModelLsts.append(modelLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.linspace(0.1, 40, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-sailing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-proxy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def DemoAndF1AndAcc(model, df):\n",
    "    f1_scores = []\n",
    "    accuracies = []\n",
    "    demographics = []\n",
    "    \n",
    "    for col in df.columns[3:]:\n",
    "        tempdf = df[(df[col] == 1)]      \n",
    "        \n",
    "        tempX = np.array(tempdf['comment_text'].values.tolist())\n",
    "        tempY = np.array(tempdf['toxicity'].values.tolist())\n",
    "        \n",
    "        predictions = model.predict(tempX)\n",
    "        \n",
    "        f1_scores.append(f1_score(tempY, predictions, zero_division=1))\n",
    "        accuracies.append(CheckAccuracy(tempY, predictions))\n",
    "        demographics.append(col)\n",
    "            \n",
    "    return np.array(demographics), np.array(f1_scores), np.array(accuracies)\n",
    "\n",
    "def pRule(model, df):\n",
    "    pRules = []\n",
    "    \n",
    "    for col in df.columns[3:]:\n",
    "        \n",
    "        tempdfz1 = df[(df[col] == 1)]      \n",
    "        tempdfz0 = df[(df[col] == 0)]\n",
    "        \n",
    "        tempXz1 = np.array(tempdfz1['comment_text'].values.tolist())\n",
    "        tempYz1 = np.array(tempdfz1['toxicity'].values.tolist())\n",
    "        \n",
    "        tempXz0 = np.array(tempdfz0['comment_text'].values.tolist())\n",
    "        tempYz0 = np.array(tempdfz0['toxicity'].values.tolist())\n",
    "        \n",
    "        predictionsz1 = model.predict(tempXz1)\n",
    "        predictionsz0 = model.predict(tempXz0)\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            \n",
    "                       \n",
    "            z1Ut1 = np.sum(predictionsz1)/len(df)\n",
    "            pz1 = len(predictionsz1)/len(df)\n",
    "            \n",
    "            z0Ut1 = np.sum(predictionsz0)/len(df)\n",
    "            pz0 = len(predictionsz0)/len(df)\n",
    "\n",
    "            pscore0 = (z1Ut1/pz1) / (z0Ut1/pz0)\n",
    "            pscore1 = (z0Ut1/pz0) / (z1Ut1/pz1)\n",
    "        \n",
    "        if np.isnan(pscore0) or np.isnan(pscore1):\n",
    "            finalpscore = 0\n",
    "        else:\n",
    "            finalpscore = min(pscore0, pscore1)\n",
    "        \n",
    "        pRules.append(finalpscore)\n",
    "    return pRules\n",
    "\n",
    "def pRuleOwn(model, df):\n",
    "    pRules = []\n",
    "    \n",
    "    for col in df.columns[3:]:\n",
    "        tempdfz1 = df[(df[col] == 1)]      \n",
    "        tempdfz0 = df[(df[col] == 0)]\n",
    "        \n",
    "        tempXz1 = np.array(tempdfz1['comment_text'].values.tolist())\n",
    "        tempYz1 = np.array(tempdfz1['toxicity'].values.tolist())\n",
    "        \n",
    "        tempXz0 = np.array(tempdfz0['comment_text'].values.tolist())\n",
    "        tempYz0 = np.array(tempdfz0['toxicity'].values.tolist())\n",
    "        \n",
    "        predictionsz1 = model.predict(tempXz1)\n",
    "        predictionsz0 = model.predict(tempXz0)\n",
    "        \n",
    "        with np.errstate(divide='ignore'):\n",
    "            pscore0 = (np.sum(predictionsz1)/np.sum(tempYz1))/(np.sum(predictionsz0)/np.sum(tempYz0))\n",
    "            pscore1 = (np.sum(predictionsz0)/np.sum(tempYz0))/(np.sum(predictionsz1)/np.sum(tempYz1))\n",
    "        \n",
    "        if np.isnan(pscore0) or np.isnan(pscore1):\n",
    "            finalpscore = 0\n",
    "        else:\n",
    "            finalpscore = min(pscore0, pscore1)\n",
    "        \n",
    "        pRules.append(finalpscore)\n",
    "    return pRules\n",
    "\n",
    "def MinMaxFairness(scores):\n",
    "    return np.max(scores)-np.min(scores)\n",
    "\n",
    "def VarianceFairness(scores):\n",
    "    return np.var(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IndividualFairness(model, df):\n",
    "    tempdf = df[(df[col] == 1)]\n",
    "    \n",
    "    X = np.array(tempdf['comment_text'].values.tolist())\n",
    "    \n",
    "    distSamples = []\n",
    "    distOutputs = []\n",
    "    \n",
    "    predict_proba = clf.predict_proba(X)[:,0]    \n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            tempDistSample = np.linalg.norm(X[i]-X[j])\n",
    "            distSamples.append(tempDistSample)\n",
    "            \n",
    "            tempDistOutput = np.linalg.norm(predict_proba[i]-predict_proba[j])\n",
    "            distOutputs.append(tempDistOutput)\n",
    "    \n",
    "    distSamples = np.array(distSamples)/np.max(distSamples)\n",
    "    distOutputs = np.array(distOutputs)/np.max(distOutputs)\n",
    "    \n",
    "    unfairScore = np.sum(distOutputs > distSamples)\n",
    "    \n",
    "    return unfairScore/len(distOutputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "#individualFairnessArr = []\n",
    "\n",
    "allvarF1 = []\n",
    "allvarAcc = []\n",
    "allminMaxF1 = []\n",
    "allminMaxAcc = []\n",
    "\n",
    "allpRuleOwnMeanArr = []\n",
    "allpRuleOwnMinArr = []\n",
    "allpRuleMeanArr = []\n",
    "allpRuleMinArr = []\n",
    "\n",
    "allaccuracy = []\n",
    "\n",
    "for modelLst in allModelLsts:\n",
    "    varF1 = []\n",
    "    varAcc = []\n",
    "    minMaxF1 = []\n",
    "    minMaxAcc = []\n",
    "    pRuleOwnMeanArr = []\n",
    "    pRuleOwnMinArr = []\n",
    "    pRuleMeanArr = []\n",
    "    pRuleMinArr = []\n",
    "    accuracy = []\n",
    "    for model in modelLst:\n",
    "        \n",
    "        demographics, f1_scores, accuracies = DemoAndF1AndAcc(model, test_data)\n",
    "        \n",
    "        own = pRuleOwn(model, test_data)\n",
    "        pRuleOwnMinArr.append(np.min(own))\n",
    "        pRuleOwnMeanArr.append(np.mean(own))\n",
    "        \n",
    "        official = pRule(model, test_data)\n",
    "        pRuleMinArr.append(np.min(official))\n",
    "        pRuleMeanArr.append(np.mean(official))\n",
    "        \n",
    "        #F1 Variance\n",
    "        varF1.append(VarianceFairness(f1_scores))\n",
    "        varAcc.append(VarianceFairness(accuracies))\n",
    "        \n",
    "        #F1 Min Max\n",
    "        minMaxF1.append(MinMaxFairness(f1_scores))\n",
    "        minMaxAcc.append(MinMaxFairness(accuracies))\n",
    "        \n",
    "        accuracy.append(model.score(X_test, Y_test))\n",
    "        \n",
    "    allvarF1.append(np.array(varF1))\n",
    "    allvarAcc.append(np.array(varAcc))\n",
    "    allminMaxF1.append(np.array(minMaxF1))\n",
    "    allminMaxAcc.append(np.array(minMaxAcc))\n",
    "    \n",
    "    allpRuleOwnMeanArr.append(np.array(pRuleOwnMeanArr))\n",
    "    allpRuleOwnMinArr.append(np.array(pRuleOwnMinArr))\n",
    "    allpRuleMeanArr.append(np.array(pRuleMeanArr))\n",
    "    allpRuleMinArr.append(np.array(pRuleMinArr))\n",
    "    \n",
    "    allaccuracy.append(np.array(accuracy))\n",
    "    \n",
    "allvarF1 = np.array(allvarF1)\n",
    "allvarAcc = np.array(allvarAcc)\n",
    "allminMaxF1 = np.array(allminMaxF1)\n",
    "allminMaxAcc = np.array(allminMaxAcc)\n",
    "allaccuracy = np.array(allaccuracy)\n",
    "\n",
    "allpRuleOwnMeanArr = np.array(allpRuleOwnMeanArr)\n",
    "allpRuleOwnMinArr = np.array(allpRuleOwnMinArr)\n",
    "allpRuleMeanArr = np.array(allpRuleMeanArr)\n",
    "allpRuleMinArr = np.array(allpRuleMinArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "allvarF1 = allvarF1.mean(axis=0)\n",
    "allvarAcc = allvarAcc.mean(axis=0)\n",
    "allminMaxF1 = allminMaxF1.mean(axis=0)\n",
    "allminMaxAcc = allminMaxAcc.mean(axis=0)\n",
    "allaccuracy = allaccuracy.mean(axis=0)\n",
    "allpRuleOwnMeanArr = allpRuleOwnMeanArr.mean(axis=0)\n",
    "allpRuleOwnMinArr = allpRuleOwnMinArr.mean(axis=0)\n",
    "allpRuleMeanArr = allpRuleMeanArr.mean(axis=0)\n",
    "allpRuleMinArr = allpRuleMinArr.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegressionModel = LR(max_iter=30000)\n",
    "LogisticRegressionModel.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "demographics, f1_scoresLR, accuracies = DemoAndF1AndAcc(LogisticRegressionModel, test_data)\n",
    "LogisticRegressionModelScore = LogisticRegressionModel.score(X_test, Y_test)\n",
    "\n",
    "own = pRuleOwn(LogisticRegressionModel, test_data)     \n",
    "pRuleOwnMin = np.min(own)\n",
    "pRuleOwnMean = np.mean(own)\n",
    "        \n",
    "official = pRule(LogisticRegressionModel, test_data)\n",
    "pRuleMin = np.min(official)\n",
    "pRuleMean = np.mean(official)\n",
    "\n",
    "\n",
    "print(\"maxmin: \", np.max(f1_scoresLR)-np.min(f1_scoresLR))\n",
    "print(\"variance fairness: \", np.var(f1_scoresLR))\n",
    "print(\"acc: \", LogisticRegressionModelScore)\n",
    "print(\"pRuleOwnMin: \", pRuleOwnMin)\n",
    "print(\"pRuleOwnMean: \", pRuleOwnMean)\n",
    "print(\"pRuleMin: \", pRuleMin)\n",
    "print(\"pRuleMean: \", pRuleMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OWN P-RULE MEAN\n",
    "\n",
    "m, b = np.polyfit(np.log(epsilons), allpRuleOwnMeanArr, 1)\n",
    "\n",
    "X2 = sm.add_constant(epsilons)\n",
    "est = sm.OLS(allpRuleOwnMeanArr, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(epsilons, allpRuleOwnMeanArr, color='r')\n",
    "plt.plot(epsilons, m*np.log(epsilons) + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * ln(x) + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Mean p%-ruleM score\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.text(18, min(allpRuleOwnMeanArr),'statistically significant with P < 0.001', fontsize=15,  color='black',style='italic', bbox={'facecolor': 'grey', 'alpha': 0.01, 'pad': 5})\n",
    "plt.legend(loc=\"upper right\", fontsize=15)\n",
    "plt.savefig('15OwnMean400models0to40eps.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OWN P-RULE MIN\n",
    "\n",
    "m, b = np.polyfit(np.log(epsilons), allpRuleOwnMinArr, 1)\n",
    "\n",
    "X2 = sm.add_constant(np.log(epsilons))\n",
    "est = sm.OLS(allpRuleOwnMinArr, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(epsilons, allpRuleOwnMinArr, color='r')\n",
    "plt.plot(epsilons, m*np.log(epsilons) + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * ln(x) + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Worst p%-ruleM score\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.text(18, min(allpRuleOwnMinArr),'statistically significant with P < 0.001', fontsize=15,  color='black',style='italic', bbox={'facecolor': 'grey', 'alpha': 0.01, 'pad': 5})\n",
    "plt.legend(loc=\"upper right\", fontsize=15)\n",
    "plt.savefig('15OwnMin400models0to40eps.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-thomson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OFFICIAL P-RULE MIN\n",
    "\n",
    "m, b = np.polyfit(np.log(epsilons), allpRuleMinArr, 1)\n",
    "\n",
    "X2 = sm.add_constant(np.log(epsilons))\n",
    "est = sm.OLS(allpRuleMinArr, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(epsilons, allpRuleMinArr, color='r')\n",
    "plt.plot(epsilons, m*np.log(epsilons) + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * ln(x) + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Worst p%-rule score\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.text(18, min(allpRuleMinArr),'statistically significant with P < 0.001', fontsize=15,  color='black',style='italic', bbox={'facecolor': 'grey', 'alpha': 0.01, 'pad': 5})\n",
    "\n",
    "plt.legend(loc=\"upper right\", fontsize=15)\n",
    "plt.savefig('15OfficialMin400models0to40eps.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OFFICIAL P-RULE Mean\n",
    "\n",
    "m, b = np.polyfit(np.log(epsilons), allpRuleMeanArr, 1)\n",
    "\n",
    "X2 = sm.add_constant(np.log(epsilons))\n",
    "est = sm.OLS(allpRuleMeanArr, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(epsilons, allpRuleMeanArr, color='r')\n",
    "plt.plot(epsilons, m*np.log(epsilons) + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * ln(x) + '+ \"{:.5f}\".format(b))\n",
    "plt.text(18, min(allpRuleMeanArr),'statistically significant with P < 0.001', fontsize=15,  color='black',style='italic', bbox={'facecolor': 'grey', 'alpha': 0.01, 'pad': 5})\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Mean p%-rule score\", fontsize=18)\n",
    "plt.legend(loc=\"upper right\", fontsize=15)\n",
    "plt.savefig('15OfficialMean400models0to40eps.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = np.polyfit(np.log(epsilons), allvarF1, 1)\n",
    "\n",
    "X2 = sm.add_constant(np.log(epsilons))\n",
    "est = sm.OLS(allvarF1, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(epsilons, allvarF1, color='r')\n",
    "plt.plot(epsilons, m*np.log(epsilons) + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * ln(x) + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.text(18, 0.00125,'statistically significant with P < 0.001', fontsize=15,  color='black',style='italic', bbox={'facecolor': 'grey', 'alpha': 0.01, 'pad': 5})\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel(\"Variance fairness\", fontsize=18)\n",
    "plt.legend(loc=\"upper right\", fontsize=15)\n",
    "\n",
    "plt.savefig('15varf1400models0to40eps.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = np.polyfit(np.log(epsilons), allminMaxF1, 1)\n",
    "\n",
    "X2 = sm.add_constant(np.log(epsilons))\n",
    "est = sm.OLS(allminMaxF1, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(epsilons, allminMaxF1, color='r')\n",
    "plt.plot(epsilons, m*np.log(epsilons) + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * ln(x) + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"maxmin fairness\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.text(18, min(allminMaxF1),'statistically significant with P < 0.001', fontsize=15,  color='black',style='italic', bbox={'facecolor': 'grey', 'alpha': 0.01, 'pad': 5})\n",
    "plt.legend(loc=\"upper right\", fontsize=15)\n",
    "plt.savefig('15minmaxf1400models0to40eps.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-serial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = np.polyfit(np.log(epsilons), allaccuracy, 1)\n",
    "\n",
    "X2 = sm.add_constant(np.log(epsilons))\n",
    "est = sm.OLS(allaccuracy, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n",
    "m, b = np.polyfit(np.log(epsilons), allaccuracy, 1)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(epsilons, allaccuracy, color='r')\n",
    "plt.plot(epsilons, m*np.log(epsilons) + b, color='b', label='f(x)='+\"{:.5f}\".format(m)+' * ln(x) + '+ \"{:.5f}\".format(b))\n",
    "plt.xlabel(\"Epsilon\", fontsize=18)\n",
    "plt.ylabel(\"Accuracy\", fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=15)\n",
    "plt.savefig('15accuracy400models0to40eps.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.npy', 'wb') as f:\n",
    "    np.save(f, epsilons)\n",
    "    np.save(f, allvarF1)\n",
    "    np.save(f, allvarAcc)\n",
    "    np.save(f, allminMaxF1)\n",
    "    np.save(f, allminMaxAcc)\n",
    "    np.save(f, allaccuracy)\n",
    "    np.save(f, allpRuleOwnMeanArr)\n",
    "    np.save(f, allpRuleOwnMinArr)\n",
    "    np.save(f, allpRuleMeanArr)\n",
    "    np.save(f, allpRuleMinArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.npy', 'rb') as f:\n",
    "    epsilons = np.load(f)\n",
    "    allvarF1 = np.load(f)\n",
    "    allvarAcc = np.load(f)\n",
    "    allminMaxF1 = np.load(f)\n",
    "    allminMaxAcc = np.load(f)\n",
    "    allaccuracy = np.load(f)\n",
    "    allpRuleOwnMeanArr = np.load(f)\n",
    "    allpRuleOwnMinArr = np.load(f)\n",
    "    allpRuleMeanArr = np.load(f)\n",
    "    allpRuleMinArr = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-healthcare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
